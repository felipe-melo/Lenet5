Using cuDNN version 5110 on context None
Mapped name None to device cuda0: GeForce GTX 980 (0000:01:00.0)
Function profiling
==================
  Message: /home/hal/Documentos/Felipe/Lenet5/lenet_5/Lenet5.py:92
  Time in 7999 calls to Function.__call__: 5.767490e+00s
  Time in Function.fn.__call__: 5.388551e+00s (93.430%)
  Time in thunks: 4.798070e+00s (83.192%)
  Total compile time: 1.907147e+01s
    Number of Apply nodes: 164
    Theano Optimizer time: 8.846152e-01s
       Theano validate time: 2.762127e-02s
    Theano Linker time (includes C, CUDA code generation/compiling): 1.816868e+01s
       Import time 8.193517e-02s
       Node make_thunk time 1.816026e+01s
           Node GpuElemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)]<gpuarray>(GpuDnnConv{algo='small', inplace=True}.0, InplaceGpuDimShuffle{x,0,x,x}.0) time 1.703783e+00s
           Node GpuElemwise{Composite{(i0 * scalar_sigmoid(i1) * (i2 - scalar_sigmoid(i1)))}}[(0, 0)]<gpuarray>(GpuReshape{4}.0, GpuElemwise{Add}[(0, 0)]<gpuarray>.0, GpuArrayConstant{[[[[ 1.]]]]}) time 1.688746e+00s
           Node GpuElemwise{Composite{(i0 * i1 * (i2 - i1))}}[(0, 0)]<gpuarray>(GpuDnnConvGradI{algo='none', inplace=True}.0, GpuElemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)]<gpuarray>.0, GpuArrayConstant{[[[[ 1.]]]]}) time 1.671800e+00s
           Node GpuElemwise{Sub}[(0, 0)]<gpuarray>(<GpuArrayType<None>(float32, (False, False, False, False))>, GpuDnnConvGradW{algo='none', inplace=True}.0) time 1.662606e+00s
           Node GpuElemwise{Add}[(0, 0)]<gpuarray>(GpuDnnConv{algo='small', inplace=True}.0, InplaceGpuDimShuffle{x,0,x,x}.0) time 1.658965e+00s

Time in all call to theano.grad() 1.888275e-02s
Time since theano import 31.031s
Class
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Class name>
  41.2%    41.2%       1.976s       8.24e-05s     C    23997       3   theano.gpuarray.basic_ops.HostFromGpu
  14.6%    55.8%       0.702s       4.39e-06s     C   159980      20   theano.gpuarray.elemwise.GpuElemwise
   5.2%    61.0%       0.249s       1.56e-05s     C    15998       2   theano.gpuarray.dnn.GpuDnnConvGradW
   4.9%    66.0%       0.237s       2.96e-05s     Py    7999       1   theano.tensor.extra_ops.Unique
   4.5%    70.5%       0.218s       4.54e-06s     C    47994       6   theano.gpuarray.basic_ops.GpuFromHost
   4.0%    74.5%       0.194s       6.05e-06s     C    31996       4   theano.gpuarray.blas.GpuDot22
   4.0%    78.5%       0.191s       1.19e-05s     C    15998       2   theano.gpuarray.dnn.GpuDnnConv
   2.9%    81.4%       0.140s       1.76e-05s     C     7999       1   theano.gpuarray.dnn.GpuDnnConvGradI
   2.9%    84.3%       0.140s       1.75e-05s     Py    7999       1   theano.tensor.basic.Dot
   2.4%    86.8%       0.117s       1.12e-06s     C   103987      13   theano.gpuarray.elemwise.GpuDimShuffle
   2.2%    89.0%       0.107s       3.34e-06s     C    31996       4   theano.gpuarray.elemwise.GpuCAReduceCuda
   1.8%    90.8%       0.088s       5.48e-06s     C    15998       2   theano.gpuarray.blas.GpuGemm
   1.4%    92.3%       0.069s       8.61e-06s     C     7999       1   theano.gpuarray.basic_ops.GpuJoin
   1.0%    93.2%       0.047s       3.91e-07s     C   119985      15   theano.tensor.elemwise.Elemwise
   0.9%    94.2%       0.045s       5.66e-06s     C     7999       1   theano.tensor.basic.Argmax
   0.9%    95.1%       0.042s       1.31e-06s     C    31996       4   theano.gpuarray.basic_ops.GpuReshape
   0.8%    95.8%       0.037s       9.14e-07s     C    39995       5   theano.gpuarray.basic_ops.GpuAllocEmpty
   0.7%    96.5%       0.033s       2.29e-07s     C   143982      18   theano.compile.ops.Shape_i
   0.6%    97.2%       0.031s       1.75e-07s     C   175978      22   theano.tensor.subtensor.Subtensor
   0.6%    97.7%       0.027s       3.36e-06s     C     7999       1   theano.gpuarray.nnet.GpuCrossentropySoftmax1HotWithBiasDx
   ... (remaining 10 Classes account for   2.28%(0.11s) of the runtime)

Ops
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Op name>
  41.2%    41.2%       1.976s       8.24e-05s     C     23997        3   HostFromGpu(gpuarray)
   5.2%    46.4%       0.249s       1.56e-05s     C     15998        2   GpuDnnConvGradW{algo='none', inplace=True}
   4.9%    51.3%       0.237s       2.96e-05s     Py    7999        1   Unique{return_index=False, return_inverse=False, return_counts=False}
   4.5%    55.9%       0.218s       4.54e-06s     C     47994        6   GpuFromHost<None>
   4.0%    59.9%       0.194s       6.05e-06s     C     31996        4   GpuDot22
   4.0%    63.9%       0.191s       1.19e-05s     C     15998        2   GpuDnnConv{algo='small', inplace=True}
   3.0%    66.9%       0.142s       3.56e-06s     C     39995        5   GpuElemwise{Composite{(i0 - (i1 * i2))}}[(0, 0)]<gpuarray>
   2.9%    69.8%       0.140s       1.76e-05s     C     7999        1   GpuDnnConvGradI{algo='none', inplace=True}
   2.9%    72.7%       0.140s       1.75e-05s     Py    7999        1   dot
   1.8%    74.5%       0.088s       5.48e-06s     C     15998        2   GpuGemm{inplace=True}
   1.5%    76.0%       0.070s       8.74e-06s     C     7999        1   GpuElemwise{Cast{int32}}[]<gpuarray>
   1.4%    77.4%       0.069s       8.61e-06s     C     7999        1   GpuJoin
   1.2%    78.7%       0.060s       3.74e-06s     C     15998        2   GpuElemwise{Add}[(0, 0)]<gpuarray>
   1.2%    79.9%       0.059s       3.72e-06s     C     15998        2   GpuCAReduceCuda{add}{0, 2, 3}
   1.2%    81.1%       0.059s       7.39e-06s     C     7999        1   GpuElemwise{Composite{Cast{int64}(EQ(i0, i1))}}[]<gpuarray>
   1.0%    82.1%       0.047s       2.97e-06s     C     15998        2   GpuCAReduceCuda{add}{0}
   0.9%    83.1%       0.045s       5.66e-06s     C     7999        1   Argmax
   0.9%    84.0%       0.045s       5.60e-06s     C     7999        1   GpuElemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)]<gpuarray>
   0.9%    84.9%       0.042s       5.27e-06s     C     7999        1   GpuElemwise{Composite{(i0 * i1 * (i2 - i1))}}[(0, 0)]<gpuarray>
   0.8%    85.6%       0.037s       9.14e-07s     C     39995        5   GpuAllocEmpty{dtype='float32', context_name=None}
   ... (remaining 45 Ops account for  14.37%(0.69s) of the runtime)

Apply
------
<% time> <sum %> <apply time> <time per call> <#call> <id> <Apply name>
  37.5%    37.5%       1.801s       2.25e-04s   7999   101   HostFromGpu(gpuarray)(GpuElemwise{Add}[(0, 0)]<gpuarray>.0)
   4.9%    42.5%       0.237s       2.96e-05s   7999   118   Unique{return_index=False, return_inverse=False, return_counts=False}(HostFromGpu(gpuarray).0)
   2.9%    45.4%       0.140s       1.76e-05s   7999   145   GpuDnnConvGradI{algo='none', inplace=True}(GpuContiguous.0, GpuContiguous.0, GpuAllocEmpty{dtype='float32', context_name=None}.0, GpuDnnConvDesc{border_mode='valid', subsample=(2, 2), conv_mode='conv', precision='float32'}.0, Constant{1.0}, Constant{0.0})
   2.9%    48.3%       0.140s       1.75e-05s   7999   144   dot(HostFromGpu(gpuarray).0, Elemwise{Composite{Cast{int64}(EQ(i0, i1))}}.0)
   2.8%    51.1%       0.136s       1.71e-05s   7999   161   GpuDnnConvGradW{algo='none', inplace=True}(GpuContiguous.0, GpuContiguous.0, GpuAllocEmpty{dtype='float32', context_name=None}.0, GpuDnnConvDesc{border_mode='valid', subsample=(2, 2), conv_mode='conv', precision='float32'}.0, Constant{1.0}, Constant{0.0})
   2.4%    53.5%       0.114s       1.43e-05s   7999    72   GpuDnnConv{algo='small', inplace=True}(GpuContiguous.0, GpuContiguous.0, GpuAllocEmpty{dtype='float32', context_name=None}.0, GpuDnnConvDesc{border_mode='valid', subsample=(2, 2), conv_mode='conv', precision='float32'}.0, Constant{1.0}, Constant{0.0})
   2.4%    55.9%       0.113s       1.41e-05s   7999   146   GpuDnnConvGradW{algo='none', inplace=True}(GpuContiguous.0, GpuContiguous.0, GpuAllocEmpty{dtype='float32', context_name=None}.0, GpuDnnConvDesc{border_mode='valid', subsample=(2, 2), conv_mode='conv', precision='float32'}.0, Constant{0.3}, Constant{0.0})
   2.1%    58.0%       0.101s       1.27e-05s   7999   141   HostFromGpu(gpuarray)(GpuElemwise{Composite{Cast{int64}(EQ(i0, i1))}}[]<gpuarray>.0)
   1.6%    59.6%       0.076s       9.56e-06s   7999    89   GpuDnnConv{algo='small', inplace=True}(GpuContiguous.0, GpuContiguous.0, GpuAllocEmpty{dtype='float32', context_name=None}.0, GpuDnnConvDesc{border_mode='valid', subsample=(2, 2), conv_mode='conv', precision='float32'}.0, Constant{1.0}, Constant{0.0})
   1.6%    61.1%       0.075s       9.33e-06s   7999   116   HostFromGpu(gpuarray)(GpuJoin.0)
   1.5%    62.6%       0.070s       8.74e-06s   7999    49   GpuElemwise{Cast{int32}}[]<gpuarray>(GpuSubtensor{int64:int64:}.0)
   1.4%    64.0%       0.069s       8.61e-06s   7999   112   GpuJoin(TensorConstant{0}, GpuElemwise{Cast{int32}}[]<gpuarray>.0, GpuFromHost<None>.0)
   1.2%    65.3%       0.059s       7.39e-06s   7999   131   GpuElemwise{Composite{Cast{int64}(EQ(i0, i1))}}[]<gpuarray>(InplaceGpuDimShuffle{x,0}.0, InplaceGpuDimShuffle{0,x}.0)
   1.2%    66.5%       0.057s       7.15e-06s   7999   103   GpuDot22(GpuCrossentropySoftmax1HotWithBiasDx.0, InplaceGpuDimShuffle{1,0}.0)
   1.2%    67.6%       0.056s       7.01e-06s   7999    93   GpuDot22(Gpusigmoid.0, W)
   1.1%    68.8%       0.055s       6.86e-06s   7999   123   GpuFromHost<None>(Unique{return_index=False, return_inverse=False, return_counts=False}.0)
   1.1%    69.8%       0.052s       6.46e-06s   7999   113   GpuGemm{inplace=True}(W, TensorConstant{-0.30000001192092896}, InplaceGpuDimShuffle{1,0}.0, GpuElemwise{Composite{(i0 * (i1 - sqr(i2)))}}[(0, 0)]<gpuarray>.0, TensorConstant{1.0})
   1.0%    70.9%       0.048s       6.03e-06s   7999   108   GpuFromHost<None>(argmax)
   0.9%    71.8%       0.045s       5.66e-06s   7999   104   Argmax(HostFromGpu(gpuarray).0, TensorConstant{(1,) of 1})
   0.9%    72.7%       0.045s       5.60e-06s   7999    73   GpuElemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)]<gpuarray>(GpuDnnConv{algo='small', inplace=True}.0, InplaceGpuDimShuffle{x,0,x,x}.0)
   ... (remaining 144 Apply instances account for 27.27%(1.31s) of the runtime)

Here are tips to potentially make your code run faster
                 (if you think of new ones, suggest them on the mailing list).
                 Test them first, as they are not guaranteed to always provide a speedup.
  - Try installing amdlibm and set the Theano flag lib.amdlibm=True. This speeds up only some Elemwise operation.
  - You have a dot operation that was not optimized to dot22 (which is faster). Make sure the inputs are float32 or float64, and are the same for both inputs. Currently they are: [TensorType(int64, matrix), TensorType(int64, matrix)]
Function profiling
==================
  Message: /home/hal/Documentos/Felipe/Lenet5/lenet_5/Lenet5.py:111
  Time in 65 calls to Function.__call__: 2.331471e-02s
  Time in Function.fn.__call__: 2.063894e-02s (88.523%)
  Time in thunks: 1.869011e-02s (80.164%)
  Total compile time: 1.177474e+00s
    Number of Apply nodes: 81
    Theano Optimizer time: 1.950514e-01s
       Theano validate time: 5.345821e-03s
    Theano Linker time (includes C, CUDA code generation/compiling): 9.717453e-01s
       Import time 1.110554e-03s
       Node make_thunk time 9.688358e-01s
           Node GpuElemwise{ScalarSigmoid}[(0, 0)]<gpuarray>(GpuReshape{2}.0) time 9.290257e-01s
           Node HostFromGpu(gpuarray)(GpuJoin.0) time 1.515865e-03s
           Node InplaceGpuDimShuffle{x,0,x,x}(<GpuArrayType<None>(float32, (False,))>) time 1.104355e-03s
           Node Elemwise{Composite{(((i0 - (((i1 - i2) * i3) + i4)) // i5) + i6)}}[(0, 0)](Shape_i{2}.0, Shape_i{2}.0, TensorConstant{1}, TensorConstant{1}, TensorConstant{1}, TensorConstant{2}, TensorConstant{1}) time 8.840561e-04s
           Node GpuElemwise{Composite{tanh((i0 + i1))}}[(0, 0)]<gpuarray>(GpuDot22.0, InplaceGpuDimShuffle{x,0}.0) time 8.709431e-04s

Time in all call to theano.grad() 1.888275e-02s
Time since theano import 31.047s
Class
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Class name>
  40.6%    40.6%       0.008s       3.89e-05s     C      195       3   theano.gpuarray.basic_ops.HostFromGpu
  12.4%    53.0%       0.002s       5.09e-06s     C      455       7   theano.gpuarray.elemwise.GpuElemwise
  10.0%    63.0%       0.002s       2.87e-05s     Py      65       1   theano.tensor.extra_ops.Unique
   8.0%    71.0%       0.001s       1.15e-05s     C      130       2   theano.gpuarray.dnn.GpuDnnConv
   5.9%    76.9%       0.001s       1.70e-05s     Py      65       1   theano.tensor.basic.Dot
   4.3%    81.2%       0.001s       6.21e-06s     C      130       2   theano.gpuarray.blas.GpuDot22
   4.3%    85.5%       0.001s       6.17e-06s     C      130       2   theano.gpuarray.basic_ops.GpuFromHost
   2.9%    88.4%       0.001s       8.39e-06s     C       65       1   theano.gpuarray.basic_ops.GpuJoin
   2.9%    91.3%       0.001s       1.17e-06s     C      455       7   theano.gpuarray.elemwise.GpuDimShuffle
   2.0%    93.3%       0.000s       3.82e-07s     C      975      15   theano.tensor.elemwise.Elemwise
   1.9%    95.2%       0.000s       5.59e-06s     C       65       1   theano.tensor.basic.Argmax
   1.5%    96.7%       0.000s       1.43e-06s     C      195       3   theano.gpuarray.basic_ops.GpuReshape
   0.7%    97.4%       0.000s       1.64e-07s     C      780      12   theano.compile.ops.Shape_i
   0.7%    98.1%       0.000s       9.56e-07s     C      130       2   theano.gpuarray.subtensor.GpuSubtensor
   0.6%    98.7%       0.000s       9.30e-07s     C      130       2   theano.gpuarray.basic_ops.GpuAllocEmpty
   0.3%    99.0%       0.000s       4.49e-07s     C      130       2   theano.tensor.elemwise.DimShuffle
   0.3%    99.3%       0.000s       3.92e-07s     C      130       2   theano.gpuarray.dnn.GpuDnnConvDesc
   0.2%    99.5%       0.000s       2.82e-07s     C      130       2   theano.compile.ops.Shape
   0.2%    99.7%       0.000s       6.46e-08s     C      520       8   theano.tensor.opt.Assert
   0.2%    99.9%       0.000s       1.27e-07s     C      260       4   theano.gpuarray.basic_ops.GpuContiguous
   ... (remaining 1 Classes account for   0.13%(0.00s) of the runtime)

Ops
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Op name>
  40.6%    40.6%       0.008s       3.89e-05s     C      195        3   HostFromGpu(gpuarray)
  10.0%    50.6%       0.002s       2.87e-05s     Py      65        1   Unique{return_index=False, return_inverse=False, return_counts=False}
   8.0%    58.6%       0.001s       1.15e-05s     C      130        2   GpuDnnConv{algo='small', inplace=True}
   5.9%    64.5%       0.001s       1.70e-05s     Py      65        1   dot
   4.3%    68.8%       0.001s       6.21e-06s     C      130        2   GpuDot22
   4.3%    73.1%       0.001s       6.17e-06s     C      130        2   GpuFromHost<None>
   2.9%    76.1%       0.001s       8.43e-06s     C       65        1   GpuElemwise{Cast{int32}}[]<gpuarray>
   2.9%    79.0%       0.001s       8.39e-06s     C       65        1   GpuJoin
   2.6%    81.6%       0.000s       3.80e-06s     C      130        2   GpuElemwise{Add}[(0, 0)]<gpuarray>
   2.5%    84.1%       0.000s       7.13e-06s     C       65        1   GpuElemwise{Composite{Cast{int64}(EQ(i0, i1))}}[]<gpuarray>
   2.0%    86.1%       0.000s       5.69e-06s     C       65        1   GpuElemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)]<gpuarray>
   1.9%    88.0%       0.000s       5.59e-06s     C       65        1   Argmax
   1.3%    89.3%       0.000s       3.69e-06s     C       65        1   GpuElemwise{Composite{tanh((i0 + i1))}}[(0, 0)]<gpuarray>
   1.1%    90.5%       0.000s       1.10e-06s     C      195        3   InplaceGpuDimShuffle{x,0}
   1.1%    91.5%       0.000s       3.11e-06s     C       65        1   GpuElemwise{ScalarSigmoid}[(0, 0)]<gpuarray>
   1.0%    92.5%       0.000s       2.81e-06s     C       65        1   Elemwise{Composite{Cast{int64}(EQ(i0, i1))}}
   0.8%    93.4%       0.000s       1.22e-06s     C      130        2   InplaceGpuDimShuffle{x,0,x,x}
   0.8%    94.1%       0.000s       2.18e-06s     C       65        1   GpuReshape{1}
   0.7%    94.8%       0.000s       9.56e-07s     C      130        2   GpuSubtensor{int64:int64:}
   0.6%    95.4%       0.000s       9.30e-07s     C      130        2   GpuAllocEmpty{dtype='float32', context_name=None}
   ... (remaining 22 Ops account for   4.57%(0.00s) of the runtime)

Apply
------
<% time> <sum %> <apply time> <time per call> <#call> <id> <Apply name>
  33.1%    33.1%       0.006s       9.50e-05s     65    67   HostFromGpu(gpuarray)(GpuElemwise{Add}[(0, 0)]<gpuarray>.0)
  10.0%    43.0%       0.002s       2.87e-05s     65    73   Unique{return_index=False, return_inverse=False, return_counts=False}(HostFromGpu(gpuarray).0)
   5.9%    49.0%       0.001s       1.70e-05s     65    80   dot(HostFromGpu(gpuarray).0, Elemwise{Composite{Cast{int64}(EQ(i0, i1))}}.0)
   4.6%    53.6%       0.001s       1.34e-05s     65    44   GpuDnnConv{algo='small', inplace=True}(GpuContiguous.0, GpuContiguous.0, GpuAllocEmpty{dtype='float32', context_name=None}.0, GpuDnnConvDesc{border_mode='valid', subsample=(2, 2), conv_mode='conv', precision='float32'}.0, Constant{1.0}, Constant{0.0})
   4.4%    58.0%       0.001s       1.26e-05s     65    79   HostFromGpu(gpuarray)(GpuElemwise{Composite{Cast{int64}(EQ(i0, i1))}}[]<gpuarray>.0)
   3.4%    61.3%       0.001s       9.68e-06s     65    59   GpuDnnConv{algo='small', inplace=True}(GpuContiguous.0, GpuContiguous.0, GpuAllocEmpty{dtype='float32', context_name=None}.0, GpuDnnConvDesc{border_mode='valid', subsample=(2, 2), conv_mode='conv', precision='float32'}.0, Constant{1.0}, Constant{0.0})
   3.2%    64.5%       0.001s       9.13e-06s     65    72   HostFromGpu(gpuarray)(GpuJoin.0)
   2.9%    67.4%       0.001s       8.43e-06s     65    28   GpuElemwise{Cast{int32}}[]<gpuarray>(GpuSubtensor{int64:int64:}.0)
   2.9%    70.4%       0.001s       8.39e-06s     65    71   GpuJoin(TensorConstant{0}, GpuElemwise{Cast{int32}}[]<gpuarray>.0, GpuFromHost<None>.0)
   2.5%    72.9%       0.000s       7.28e-06s     65    63   GpuDot22(GpuElemwise{ScalarSigmoid}[(0, 0)]<gpuarray>.0, W)
   2.5%    75.4%       0.000s       7.13e-06s     65    78   GpuElemwise{Composite{Cast{int64}(EQ(i0, i1))}}[]<gpuarray>(InplaceGpuDimShuffle{x,0}.0, InplaceGpuDimShuffle{0,x}.0)
   2.3%    77.7%       0.000s       6.54e-06s     65    75   GpuFromHost<None>(Unique{return_index=False, return_inverse=False, return_counts=False}.0)
   2.0%    79.7%       0.000s       5.79e-06s     65    69   GpuFromHost<None>(argmax)
   2.0%    81.6%       0.000s       5.69e-06s     65    45   GpuElemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)]<gpuarray>(GpuDnnConv{algo='small', inplace=True}.0, InplaceGpuDimShuffle{x,0,x,x}.0)
   1.9%    83.6%       0.000s       5.59e-06s     65    68   Argmax(HostFromGpu(gpuarray).0, TensorConstant{(1,) of 1})
   1.8%    85.4%       0.000s       5.14e-06s     65    65   GpuDot22(GpuElemwise{Composite{tanh((i0 + i1))}}[(0, 0)]<gpuarray>.0, W)
   1.5%    86.8%       0.000s       4.19e-06s     65    60   GpuElemwise{Add}[(0, 0)]<gpuarray>(GpuDnnConv{algo='small', inplace=True}.0, InplaceGpuDimShuffle{x,0,x,x}.0)
   1.3%    88.1%       0.000s       3.69e-06s     65    64   GpuElemwise{Composite{tanh((i0 + i1))}}[(0, 0)]<gpuarray>(GpuDot22.0, InplaceGpuDimShuffle{x,0}.0)
   1.2%    89.3%       0.000s       3.41e-06s     65    66   GpuElemwise{Add}[(0, 0)]<gpuarray>(GpuDot22.0, InplaceGpuDimShuffle{x,0}.0)
   1.1%    90.4%       0.000s       3.11e-06s     65    62   GpuElemwise{ScalarSigmoid}[(0, 0)]<gpuarray>(GpuReshape{2}.0)
   ... (remaining 61 Apply instances account for 9.61%(0.00s) of the runtime)

Here are tips to potentially make your code run faster
                 (if you think of new ones, suggest them on the mailing list).
                 Test them first, as they are not guaranteed to always provide a speedup.
  - Try installing amdlibm and set the Theano flag lib.amdlibm=True. This speeds up only some Elemwise operation.
  - You have a dot operation that was not optimized to dot22 (which is faster). Make sure the inputs are float32 or float64, and are the same for both inputs. Currently they are: [TensorType(int64, matrix), TensorType(int64, matrix)]
Function profiling
==================
  Message: Sum of all(2) printed profiles at exit excluding Scan op profile.
  Time in 8064 calls to Function.__call__: 5.790805e+00s
  Time in Function.fn.__call__: 5.409190e+00s (93.410%)
  Time in thunks: 4.816760e+00s (83.179%)
  Total compile time: 2.024895e+01s
    Number of Apply nodes: 164
    Theano Optimizer time: 1.079667e+00s
       Theano validate time: 3.296709e-02s
    Theano Linker time (includes C, CUDA code generation/compiling): 1.914042e+01s
       Import time 8.304572e-02s
       Node make_thunk time 1.912910e+01s
           Node GpuElemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)]<gpuarray>(GpuDnnConv{algo='small', inplace=True}.0, InplaceGpuDimShuffle{x,0,x,x}.0) time 1.703783e+00s
           Node GpuElemwise{Composite{(i0 * scalar_sigmoid(i1) * (i2 - scalar_sigmoid(i1)))}}[(0, 0)]<gpuarray>(GpuReshape{4}.0, GpuElemwise{Add}[(0, 0)]<gpuarray>.0, GpuArrayConstant{[[[[ 1.]]]]}) time 1.688746e+00s
           Node GpuElemwise{Composite{(i0 * i1 * (i2 - i1))}}[(0, 0)]<gpuarray>(GpuDnnConvGradI{algo='none', inplace=True}.0, GpuElemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)]<gpuarray>.0, GpuArrayConstant{[[[[ 1.]]]]}) time 1.671800e+00s
           Node GpuElemwise{Sub}[(0, 0)]<gpuarray>(<GpuArrayType<None>(float32, (False, False, False, False))>, GpuDnnConvGradW{algo='none', inplace=True}.0) time 1.662606e+00s
           Node GpuElemwise{Add}[(0, 0)]<gpuarray>(GpuDnnConv{algo='small', inplace=True}.0, InplaceGpuDimShuffle{x,0,x,x}.0) time 1.658965e+00s

Time in all call to theano.grad() 1.888275e-02s
Time since theano import 31.055s
Class
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Class name>
  41.2%    41.2%       1.984s       8.20e-05s     C    24192       6   theano.gpuarray.basic_ops.HostFromGpu
  14.6%    55.8%       0.704s       4.39e-06s     C   160435      27   theano.gpuarray.elemwise.GpuElemwise
   5.2%    61.0%       0.249s       1.56e-05s     C    15998       2   theano.gpuarray.dnn.GpuDnnConvGradW
   5.0%    65.9%       0.239s       2.96e-05s     Py    8064       2   theano.tensor.extra_ops.Unique
   4.5%    70.5%       0.219s       4.54e-06s     C    48124       8   theano.gpuarray.basic_ops.GpuFromHost
   4.0%    74.5%       0.194s       6.05e-06s     C    32126       6   theano.gpuarray.blas.GpuDot22
   4.0%    78.5%       0.192s       1.19e-05s     C    16128       4   theano.gpuarray.dnn.GpuDnnConv
   2.9%    81.4%       0.141s       1.74e-05s     Py    8064       2   theano.tensor.basic.Dot
   2.9%    84.3%       0.140s       1.76e-05s     C     7999       1   theano.gpuarray.dnn.GpuDnnConvGradI
   2.4%    86.8%       0.117s       1.12e-06s     C   104442      20   theano.gpuarray.elemwise.GpuDimShuffle
   2.2%    89.0%       0.107s       3.34e-06s     C    31996       4   theano.gpuarray.elemwise.GpuCAReduceCuda
   1.8%    90.8%       0.088s       5.48e-06s     C    15998       2   theano.gpuarray.blas.GpuGemm
   1.4%    92.3%       0.069s       8.60e-06s     C     8064       2   theano.gpuarray.basic_ops.GpuJoin
   1.0%    93.2%       0.047s       3.91e-07s     C   120960      30   theano.tensor.elemwise.Elemwise
   0.9%    94.2%       0.046s       5.66e-06s     C     8064       2   theano.tensor.basic.Argmax
   0.9%    95.1%       0.042s       1.31e-06s     C    32191       7   theano.gpuarray.basic_ops.GpuReshape
   0.8%    95.8%       0.037s       9.14e-07s     C    40125       7   theano.gpuarray.basic_ops.GpuAllocEmpty
   0.7%    96.5%       0.033s       2.28e-07s     C   144762      30   theano.compile.ops.Shape_i
   0.6%    97.2%       0.031s       1.75e-07s     C   175978      22   theano.tensor.subtensor.Subtensor
   0.6%    97.7%       0.027s       3.36e-06s     C     7999       1   theano.gpuarray.nnet.GpuCrossentropySoftmax1HotWithBiasDx
   ... (remaining 10 Classes account for   2.28%(0.11s) of the runtime)

Ops
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Op name>
  41.2%    41.2%       1.984s       8.20e-05s     C     24192        6   HostFromGpu(gpuarray)
   5.2%    46.4%       0.249s       1.56e-05s     C     15998        2   GpuDnnConvGradW{algo='none', inplace=True}
   5.0%    51.3%       0.239s       2.96e-05s     Py    8064        2   Unique{return_index=False, return_inverse=False, return_counts=False}
   4.5%    55.9%       0.219s       4.54e-06s     C     48124        8   GpuFromHost<None>
   4.0%    59.9%       0.194s       6.05e-06s     C     32126        6   GpuDot22
   4.0%    63.9%       0.192s       1.19e-05s     C     16128        4   GpuDnnConv{algo='small', inplace=True}
   3.0%    66.9%       0.142s       3.56e-06s     C     39995        5   GpuElemwise{Composite{(i0 - (i1 * i2))}}[(0, 0)]<gpuarray>
   2.9%    69.8%       0.141s       1.74e-05s     Py    8064        2   dot
   2.9%    72.7%       0.140s       1.76e-05s     C     7999        1   GpuDnnConvGradI{algo='none', inplace=True}
   1.8%    74.5%       0.088s       5.48e-06s     C     15998        2   GpuGemm{inplace=True}
   1.5%    76.0%       0.070s       8.74e-06s     C     8064        2   GpuElemwise{Cast{int32}}[]<gpuarray>
   1.4%    77.4%       0.069s       8.60e-06s     C     8064        2   GpuJoin
   1.3%    78.7%       0.060s       3.74e-06s     C     16128        4   GpuElemwise{Add}[(0, 0)]<gpuarray>
   1.2%    79.9%       0.060s       7.39e-06s     C     8064        2   GpuElemwise{Composite{Cast{int64}(EQ(i0, i1))}}[]<gpuarray>
   1.2%    81.1%       0.059s       3.72e-06s     C     15998        2   GpuCAReduceCuda{add}{0, 2, 3}
   1.0%    82.1%       0.047s       2.97e-06s     C     15998        2   GpuCAReduceCuda{add}{0}
   0.9%    83.1%       0.046s       5.66e-06s     C     8064        2   Argmax
   0.9%    84.0%       0.045s       5.60e-06s     C     8064        2   GpuElemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)]<gpuarray>
   0.9%    84.9%       0.042s       5.27e-06s     C     7999        1   GpuElemwise{Composite{(i0 * i1 * (i2 - i1))}}[(0, 0)]<gpuarray>
   0.8%    85.6%       0.037s       9.14e-07s     C     40125        7   GpuAllocEmpty{dtype='float32', context_name=None}
   ... (remaining 46 Ops account for  14.36%(0.69s) of the runtime)

Apply
------
<% time> <sum %> <apply time> <time per call> <#call> <id> <Apply name>
  37.4%    37.4%       1.801s       2.25e-04s   7999   101   HostFromGpu(gpuarray)(GpuElemwise{Add}[(0, 0)]<gpuarray>.0)
   4.9%    42.3%       0.237s       2.96e-05s   7999   118   Unique{return_index=False, return_inverse=False, return_counts=False}(HostFromGpu(gpuarray).0)
   2.9%    45.2%       0.140s       1.76e-05s   7999   145   GpuDnnConvGradI{algo='none', inplace=True}(GpuContiguous.0, GpuContiguous.0, GpuAllocEmpty{dtype='float32', context_name=None}.0, GpuDnnConvDesc{border_mode='valid', subsample=(2, 2), conv_mode='conv', precision='float32'}.0, Constant{1.0}, Constant{0.0})
   2.9%    48.1%       0.140s       1.75e-05s   7999   144   dot(HostFromGpu(gpuarray).0, Elemwise{Composite{Cast{int64}(EQ(i0, i1))}}.0)
   2.8%    50.9%       0.136s       1.71e-05s   7999   161   GpuDnnConvGradW{algo='none', inplace=True}(GpuContiguous.0, GpuContiguous.0, GpuAllocEmpty{dtype='float32', context_name=None}.0, GpuDnnConvDesc{border_mode='valid', subsample=(2, 2), conv_mode='conv', precision='float32'}.0, Constant{1.0}, Constant{0.0})
   2.4%    53.3%       0.114s       1.43e-05s   7999    72   GpuDnnConv{algo='small', inplace=True}(GpuContiguous.0, GpuContiguous.0, GpuAllocEmpty{dtype='float32', context_name=None}.0, GpuDnnConvDesc{border_mode='valid', subsample=(2, 2), conv_mode='conv', precision='float32'}.0, Constant{1.0}, Constant{0.0})
   2.3%    55.7%       0.113s       1.41e-05s   7999   146   GpuDnnConvGradW{algo='none', inplace=True}(GpuContiguous.0, GpuContiguous.0, GpuAllocEmpty{dtype='float32', context_name=None}.0, GpuDnnConvDesc{border_mode='valid', subsample=(2, 2), conv_mode='conv', precision='float32'}.0, Constant{0.3}, Constant{0.0})
   2.1%    57.8%       0.101s       1.27e-05s   7999   141   HostFromGpu(gpuarray)(GpuElemwise{Composite{Cast{int64}(EQ(i0, i1))}}[]<gpuarray>.0)
   1.6%    59.4%       0.076s       9.56e-06s   7999    89   GpuDnnConv{algo='small', inplace=True}(GpuContiguous.0, GpuContiguous.0, GpuAllocEmpty{dtype='float32', context_name=None}.0, GpuDnnConvDesc{border_mode='valid', subsample=(2, 2), conv_mode='conv', precision='float32'}.0, Constant{1.0}, Constant{0.0})
   1.5%    60.9%       0.075s       9.33e-06s   7999   116   HostFromGpu(gpuarray)(GpuJoin.0)
   1.5%    62.4%       0.070s       8.74e-06s   7999    49   GpuElemwise{Cast{int32}}[]<gpuarray>(GpuSubtensor{int64:int64:}.0)
   1.4%    63.8%       0.069s       8.61e-06s   7999   112   GpuJoin(TensorConstant{0}, GpuElemwise{Cast{int32}}[]<gpuarray>.0, GpuFromHost<None>.0)
   1.2%    65.0%       0.059s       7.39e-06s   7999   131   GpuElemwise{Composite{Cast{int64}(EQ(i0, i1))}}[]<gpuarray>(InplaceGpuDimShuffle{x,0}.0, InplaceGpuDimShuffle{0,x}.0)
   1.2%    66.2%       0.057s       7.15e-06s   7999   103   GpuDot22(GpuCrossentropySoftmax1HotWithBiasDx.0, InplaceGpuDimShuffle{1,0}.0)
   1.2%    67.4%       0.056s       7.01e-06s   7999    93   GpuDot22(Gpusigmoid.0, W)
   1.1%    68.5%       0.055s       6.86e-06s   7999   123   GpuFromHost<None>(Unique{return_index=False, return_inverse=False, return_counts=False}.0)
   1.1%    69.6%       0.052s       6.46e-06s   7999   113   GpuGemm{inplace=True}(W, TensorConstant{-0.30000001192092896}, InplaceGpuDimShuffle{1,0}.0, GpuElemwise{Composite{(i0 * (i1 - sqr(i2)))}}[(0, 0)]<gpuarray>.0, TensorConstant{1.0})
   1.0%    70.6%       0.048s       6.03e-06s   7999   108   GpuFromHost<None>(argmax)
   0.9%    71.5%       0.045s       5.66e-06s   7999   104   Argmax(HostFromGpu(gpuarray).0, TensorConstant{(1,) of 1})
   0.9%    72.4%       0.045s       5.60e-06s   7999    73   GpuElemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)]<gpuarray>(GpuDnnConv{algo='small', inplace=True}.0, InplaceGpuDimShuffle{x,0,x,x}.0)
   ... (remaining 225 Apply instances account for 27.56%(1.33s) of the runtime)

Here are tips to potentially make your code run faster
                 (if you think of new ones, suggest them on the mailing list).
                 Test them first, as they are not guaranteed to always provide a speedup.
  - Try installing amdlibm and set the Theano flag lib.amdlibm=True. This speeds up only some Elemwise operation.
  - You have a dot operation that was not optimized to dot22 (which is faster). Make sure the inputs are float32 or float64, and are the same for both inputs. Currently they are: [TensorType(int64, matrix), TensorType(int64, matrix)]
  - You have a dot operation that was not optimized to dot22 (which is faster). Make sure the inputs are float32 or float64, and are the same for both inputs. Currently they are: [TensorType(int64, matrix), TensorType(int64, matrix)]
Function profiling
==================
  Message: /home/hal/Documentos/Felipe/Lenet5/lenet_5/Lenet5.py:92
  Time in 7999 calls to Function.__call__: 1.123110e+02s
  Time in Function.fn.__call__: 1.117634e+02s (99.512%)
  Time in thunks: 1.113789e+02s (99.170%)
  Total compile time: 1.076749e+00s
    Number of Apply nodes: 86
    Theano Optimizer time: 6.481097e-01s
       Theano validate time: 1.235676e-02s
    Theano Linker time (includes C, CUDA code generation/compiling): 6.534529e-02s
       Import time 1.929021e-02s
       Node make_thunk time 6.158495e-02s
           Node Elemwise{Composite{(i0 - (i1 * i2))}}[(0, 0)](<TensorType(float32, 4D)>, TensorConstant{(1, 1, 1, 1) of 0.3}, Subtensor{::, ::, ::int64, ::int64}.0) time 1.805305e-03s
           Node Elemwise{Composite{Switch(LT(Composite{Switch(GE(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(LT(i0, i1), (i0 + i2), i0)}(i0, i1, i2), i1, i3), i2), i1), i1, Composite{Switch(GE(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(LT(i0, i1), (i0 + i2), i0)}(i0, i1, i2), i1, i3), i2))}}[(0, 0)](Elemwise{Composite{(i0 * (i1 + i2))}}.0, TensorConstant{0}, Elemwise{Mul}[(0, 0)].0, TensorConstant{-1}) time 1.278877e-03s
           Node Elemwise{Composite{Switch(LT(Composite{Switch(GE(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), (i0 + i2), i0)}(i0, i1, i2), i1), i2), i1), i1, Composite{Switch(GE(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), (i0 + i2), i0)}(i0, i1, i2), i1), i2))}}[(0, 0)](Elemwise{mul,no_inplace}.0, TensorConstant{0}, Elemwise{Mul}[(0, 0)].0) time 1.208544e-03s
           Node Elemwise{Composite{(i0 * (i1 - sqr(i2)))}}[(0, 0)](Dot22.0, TensorConstant{(1, 1) of 1.0}, Elemwise{Composite{tanh((i0 + i1))}}[(0, 0)].0) time 1.158476e-03s
           Node InplaceDimShuffle{x,0}(b) time 1.150846e-03s

Time in all call to theano.grad() 1.684117e-02s
Time since theano import 115.838s
Class
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Class name>
  46.2%    46.2%      51.459s       2.80e-04s     C   183977      23   theano.tensor.elemwise.Elemwise
  19.8%    66.0%      22.052s       1.38e-03s     C    15998       2   theano.tensor.nnet.corr.CorrMM_gradWeights
  17.7%    83.7%      19.712s       1.23e-03s     C    15998       2   theano.tensor.nnet.corr.CorrMM
  11.8%    95.5%      13.186s       1.65e-03s     C     7999       1   theano.tensor.nnet.corr.CorrMM_gradInputs
   2.0%    97.6%       2.259s       7.06e-05s     C    31996       4   theano.tensor.elemwise.Sum
   1.1%    98.6%       1.182s       3.70e-05s     C    31996       4   theano.tensor.blas.Dot22
   0.4%    99.1%       0.474s       2.96e-05s     C    15998       2   theano.tensor.blas.Gemm
   0.3%    99.3%       0.284s       3.55e-05s     C     7999       1   theano.tensor.nnet.nnet.SoftmaxWithBias
   0.3%    99.6%       0.279s       3.48e-05s     Py    7999       1   theano.tensor.extra_ops.Unique
   0.1%    99.7%       0.141s       1.76e-05s     Py    7999       1   theano.tensor.basic.Dot
   0.1%    99.8%       0.108s       3.37e-06s     PyC    31996       4   theano.tensor.basic.Reshape
   0.1%    99.8%       0.056s       4.67e-07s     C   119985      15   theano.tensor.elemwise.DimShuffle
   0.0%    99.9%       0.053s       6.68e-06s     C     7999       1   theano.tensor.basic.Argmax
   0.0%    99.9%       0.053s       5.55e-07s     C    95988      12   theano.tensor.subtensor.Subtensor
   0.0%   100.0%       0.030s       3.78e-06s     C     7999       1   theano.tensor.basic.Join
   0.0%   100.0%       0.019s       2.37e-06s     C     7999       1   theano.tensor.nnet.nnet.CrossentropySoftmax1HotWithBiasDx
   0.0%   100.0%       0.018s       3.81e-07s     C    47994       6   theano.compile.ops.Shape_i
   0.0%   100.0%       0.009s       5.36e-07s     C    15998       2   theano.tensor.opt.MakeVector
   0.0%   100.0%       0.003s       1.75e-07s     C    15998       2   theano.tensor.basic.ScalarFromTensor
   0.0%   100.0%       0.002s       3.00e-07s     C     7999       1   theano.compile.ops.Rebroadcast
   ... (remaining 0 Classes account for   0.00%(0.00s) of the runtime)

Ops
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Op name>
  25.0%    25.0%      27.830s       3.48e-03s     C     7999        1   Elemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)]
  19.8%    44.8%      22.052s       1.38e-03s     C     15998        2   CorrMM_gradWeights{valid, (2, 2), (1, 1)}
  17.7%    62.5%      19.712s       1.23e-03s     C     15998        2   CorrMM{valid, (2, 2), (1, 1)}
  11.8%    74.3%      13.186s       1.65e-03s     C     7999        1   CorrMM_gradInputs{valid, (2, 2), (1, 1)}
   8.7%    83.0%       9.704s       1.21e-03s     C     7999        1   Elemwise{Composite{(i0 * scalar_sigmoid(i1) * (i2 - scalar_sigmoid(i1)))}}[(0, 0)]
   8.2%    91.3%       9.153s       1.14e-03s     C     7999        1   sigmoid
   3.3%    94.6%       3.722s       4.65e-04s     C     7999        1   Elemwise{Composite{tanh((i0 + i1))}}[(0, 0)]
   1.9%    96.5%       2.114s       1.32e-04s     C     15998        2   Sum{axis=[0, 2, 3], acc_dtype=float64}
   1.1%    97.6%       1.182s       3.70e-05s     C     31996        4   Dot22
   0.4%    98.0%       0.489s       3.06e-05s     C     15998        2   Elemwise{Add}[(0, 0)]
   0.4%    98.4%       0.474s       2.96e-05s     C     15998        2   Gemm{inplace}
   0.3%    98.7%       0.367s       4.59e-05s     C     7999        1   Elemwise{Composite{(i0 * i1 * (i2 - i1))}}[(0, 0)]
   0.3%    99.0%       0.284s       3.55e-05s     C     7999        1   SoftmaxWithBias
   0.3%    99.3%       0.279s       3.48e-05s     Py    7999        1   Unique{return_index=False, return_inverse=False, return_counts=False}
   0.1%    99.4%       0.145s       9.06e-06s     C     15998        2   Sum{axis=[0], acc_dtype=float64}
   0.1%    99.5%       0.141s       1.76e-05s     Py    7999        1   dot
   0.1%    99.6%       0.089s       1.11e-05s     Py    7999        1   Reshape{1}
   0.1%    99.6%       0.056s       1.18e-06s     C     47994        6   Elemwise{Composite{(i0 - (i1 * i2))}}[(0, 0)]
   0.0%    99.7%       0.055s       6.92e-06s     C     7999        1   Elemwise{Composite{(i0 * (i1 - sqr(i2)))}}[(0, 0)]
   0.0%    99.7%       0.053s       6.68e-06s     C     7999        1   Argmax
   ... (remaining 29 Ops account for   0.26%(0.29s) of the runtime)

Apply
------
<% time> <sum %> <apply time> <time per call> <#call> <id> <Apply name>
  25.0%    25.0%      27.830s       3.48e-03s   7999    40   Elemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)](CorrMM{valid, (2, 2), (1, 1)}.0, InplaceDimShuffle{x,0,x,x}.0)
  11.8%    36.8%      13.186s       1.65e-03s   7999    74   CorrMM_gradInputs{valid, (2, 2), (1, 1)}(Subtensor{::, ::, ::int64, ::int64}.0, Elemwise{Composite{(i0 * scalar_sigmoid(i1) * (i2 - scalar_sigmoid(i1)))}}[(0, 0)].0, Subtensor{int64}.0, Subtensor{int64}.0)
  10.4%    47.2%      11.548s       1.44e-03s   7999    75   CorrMM_gradWeights{valid, (2, 2), (1, 1)}(Elemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)].0, Elemwise{Composite{(i0 * scalar_sigmoid(i1) * (i2 - scalar_sigmoid(i1)))}}[(0, 0)].0, Subtensor{int64}.0, Subtensor{int64}.0)
  10.0%    57.2%      11.175s       1.40e-03s   7999    41   CorrMM{valid, (2, 2), (1, 1)}(Elemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)].0, Subtensor{::, ::, ::int64, ::int64}.0)
   9.4%    66.7%      10.504s       1.31e-03s   7999    80   CorrMM_gradWeights{valid, (2, 2), (1, 1)}(InplaceDimShuffle{0,x,1,2}.0, Elemwise{Composite{(i0 * i1 * (i2 - i1))}}[(0, 0)].0, Subtensor{int64}.0, Subtensor{int64}.0)
   8.7%    75.4%       9.704s       1.21e-03s   7999    71   Elemwise{Composite{(i0 * scalar_sigmoid(i1) * (i2 - scalar_sigmoid(i1)))}}[(0, 0)](Reshape{4}.0, Elemwise{Add}[(0, 0)].0, TensorConstant{(1, 1, 1, 1) of 1.0})
   8.2%    83.6%       9.153s       1.14e-03s   7999    44   sigmoid(Reshape{2}.0)
   7.7%    91.3%       8.537s       1.07e-03s   7999    39   CorrMM{valid, (2, 2), (1, 1)}(InplaceDimShuffle{0,x,1,2}.0, Subtensor{::, ::, ::int64, ::int64}.0)
   3.3%    94.6%       3.722s       4.65e-04s   7999    47   Elemwise{Composite{tanh((i0 + i1))}}[(0, 0)](Dot22.0, InplaceDimShuffle{x,0}.0)
   1.3%    95.9%       1.492s       1.86e-04s   7999    79   Sum{axis=[0, 2, 3], acc_dtype=float64}(Elemwise{Composite{(i0 * i1 * (i2 - i1))}}[(0, 0)].0)
   0.6%    96.5%       0.622s       7.78e-05s   7999    73   Sum{axis=[0, 2, 3], acc_dtype=float64}(Elemwise{Composite{(i0 * scalar_sigmoid(i1) * (i2 - scalar_sigmoid(i1)))}}[(0, 0)].0)
   0.5%    97.0%       0.587s       7.34e-05s   7999    45   Dot22(sigmoid.0, W)
   0.4%    97.4%       0.473s       5.91e-05s   7999    42   Elemwise{Add}[(0, 0)](CorrMM{valid, (2, 2), (1, 1)}.0, InplaceDimShuffle{x,0,x,x}.0)
   0.4%    97.9%       0.469s       5.86e-05s   7999    63   Dot22(Elemwise{Composite{(i0 * (i1 - sqr(i2)))}}[(0, 0)].0, W.T)
   0.4%    98.2%       0.422s       5.28e-05s   7999    66   Gemm{inplace}(W, TensorConstant{-0.30000001192092896}, InplaceDimShuffle{1,0}.0, Elemwise{Composite{(i0 * (i1 - sqr(i2)))}}[(0, 0)].0, TensorConstant{1.0})
   0.3%    98.6%       0.367s       4.59e-05s   7999    77   Elemwise{Composite{(i0 * i1 * (i2 - i1))}}[(0, 0)](CorrMM_gradInputs{valid, (2, 2), (1, 1)}.0, Elemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)].0, TensorConstant{(1, 1, 1, 1) of 1.0})
   0.3%    98.8%       0.284s       3.55e-05s   7999    50   SoftmaxWithBias(Dot22.0, b)
   0.3%    99.1%       0.279s       3.48e-05s   7999    61   Unique{return_index=False, return_inverse=False, return_counts=False}(Join.0)
   0.1%    99.2%       0.141s       1.76e-05s   7999    72   dot(Elemwise{Composite{Cast{int64}(EQ(i0, i1))}}.0, Elemwise{Composite{Cast{int64}(EQ(i0, i1))}}.0)
   0.1%    99.3%       0.129s       1.61e-05s   7999    62   Sum{axis=[0], acc_dtype=float64}(Elemwise{Composite{(i0 * (i1 - sqr(i2)))}}[(0, 0)].0)
   ... (remaining 66 Apply instances account for 0.68%(0.76s) of the runtime)

Here are tips to potentially make your code run faster
                 (if you think of new ones, suggest them on the mailing list).
                 Test them first, as they are not guaranteed to always provide a speedup.
  - Try installing amdlibm and set the Theano flag lib.amdlibm=True. This speeds up only some Elemwise operation.
  - You have a dot operation that was not optimized to dot22 (which is faster). Make sure the inputs are float32 or float64, and are the same for both inputs. Currently they are: [TensorType(int64, matrix), TensorType(int64, matrix)]
Function profiling
==================
  Message: /home/hal/Documentos/Felipe/Lenet5/lenet_5/Lenet5.py:111
  Time in 65 calls to Function.__call__: 4.971251e-01s
  Time in Function.fn.__call__: 4.941759e-01s (99.407%)
  Time in thunks: 4.930990e-01s (99.190%)
  Total compile time: 1.361289e-01s
    Number of Apply nodes: 36
    Theano Optimizer time: 1.102974e-01s
       Theano validate time: 1.546144e-03s
    Theano Linker time (includes C, CUDA code generation/compiling): 1.725841e-02s
       Import time 1.001835e-03s
       Node make_thunk time 1.597023e-02s
           Node Elemwise{ScalarSigmoid}[(0, 0)](Reshape{2}.0) time 9.658337e-04s
           Node InplaceDimShuffle{x,0,x,x}(<TensorType(float32, vector)>) time 9.486675e-04s
           Node Elemwise{Composite{(i0 * (i1 + i2))}}(TensorConstant{150}, TensorConstant{1}, <TensorType(int64, scalar)>) time 6.670952e-04s
           Node Elemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)](CorrMM{valid, (2, 2), (1, 1)}.0, InplaceDimShuffle{x,0,x,x}.0) time 5.679131e-04s
           Node Reshape{1}(<TensorType(float32, matrix)>, TensorConstant{(1,) of -1}) time 5.247593e-04s

Time in all call to theano.grad() 1.684117e-02s
Time since theano import 115.854s
Class
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Class name>
  65.6%    65.6%       0.324s       4.98e-04s     C      650      10   theano.tensor.elemwise.Elemwise
  32.5%    98.1%       0.160s       1.23e-03s     C      130       2   theano.tensor.nnet.corr.CorrMM
   1.0%    99.1%       0.005s       3.72e-05s     C      130       2   theano.tensor.blas.Dot22
   0.4%    99.5%       0.002s       2.96e-05s     Py      65       1   theano.tensor.extra_ops.Unique
   0.2%    99.7%       0.001s       1.63e-05s     Py      65       1   theano.tensor.basic.Dot
   0.1%    99.8%       0.001s       2.93e-06s     PyC      195       3   theano.tensor.basic.Reshape
   0.1%    99.9%       0.000s       5.83e-06s     C       65       1   theano.tensor.basic.Argmax
   0.0%    99.9%       0.000s       3.09e-06s     C       65       1   theano.tensor.basic.Join
   0.0%   100.0%       0.000s       3.35e-07s     C      585       9   theano.tensor.elemwise.DimShuffle
   0.0%   100.0%       0.000s       6.00e-07s     C      260       4   theano.tensor.subtensor.Subtensor
   0.0%   100.0%       0.000s       1.43e-07s     C      130       2   theano.tensor.basic.ScalarFromTensor
   ... (remaining 0 Classes account for   0.00%(0.00s) of the runtime)

Ops
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Op name>
  43.8%    43.8%       0.216s       3.32e-03s     C       65        1   Elemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)]
  32.5%    76.3%       0.160s       1.23e-03s     C      130        2   CorrMM{valid, (2, 2), (1, 1)}
  14.6%    90.9%       0.072s       1.11e-03s     C       65        1   Elemwise{ScalarSigmoid}[(0, 0)]
   6.3%    97.2%       0.031s       4.81e-04s     C       65        1   Elemwise{Composite{tanh((i0 + i1))}}[(0, 0)]
   1.0%    98.2%       0.005s       3.72e-05s     C      130        2   Dot22
   0.8%    99.0%       0.004s       3.04e-05s     C      130        2   Elemwise{Add}[(0, 0)]
   0.4%    99.4%       0.002s       2.96e-05s     Py      65        1   Unique{return_index=False, return_inverse=False, return_counts=False}
   0.2%    99.6%       0.001s       1.63e-05s     Py      65        1   dot
   0.1%    99.7%       0.001s       7.72e-06s     Py      65        1   Reshape{1}
   0.1%    99.8%       0.000s       5.83e-06s     C       65        1   Argmax
   0.1%    99.8%       0.000s       2.65e-06s     C      130        2   Elemwise{Composite{Cast{int64}(EQ(i0, i1))}}
   0.0%    99.9%       0.000s       3.09e-06s     C       65        1   Join
   0.0%    99.9%       0.000s       7.06e-07s     C      130        2   Subtensor{::, ::, ::int64, ::int64}
   0.0%    99.9%       0.000s       2.52e-07s     C      260        4   InplaceDimShuffle{x,0}
   0.0%    99.9%       0.000s       4.93e-07s     C      130        2   Subtensor{int64:int64:}
   0.0%    99.9%       0.000s       4.60e-07s     C      130        2   InplaceDimShuffle{x,0,x,x}
   0.0%   100.0%       0.000s       6.20e-07s     C       65        1   Reshape{2}
   0.0%   100.0%       0.000s       3.08e-07s     C      130        2   InplaceDimShuffle{0,x}
   0.0%   100.0%       0.000s       5.43e-07s     C       65        1   Elemwise{Cast{int32}}
   0.0%   100.0%       0.000s       5.32e-07s     C       65        1   Elemwise{mul,no_inplace}
   ... (remaining 4 Ops account for   0.02%(0.00s) of the runtime)

Apply
------
<% time> <sum %> <apply time> <time per call> <#call> <id> <Apply name>
  43.8%    43.8%       0.216s       3.32e-03s     65    18   Elemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)](CorrMM{valid, (2, 2), (1, 1)}.0, InplaceDimShuffle{x,0,x,x}.0)
  18.4%    62.3%       0.091s       1.40e-03s     65    19   CorrMM{valid, (2, 2), (1, 1)}(Elemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)].0, Subtensor{::, ::, ::int64, ::int64}.0)
  14.6%    76.8%       0.072s       1.11e-03s     65    22   Elemwise{ScalarSigmoid}[(0, 0)](Reshape{2}.0)
  14.0%    90.9%       0.069s       1.06e-03s     65    17   CorrMM{valid, (2, 2), (1, 1)}(InplaceDimShuffle{0,x,1,2}.0, Subtensor{::, ::, ::int64, ::int64}.0)
   6.3%    97.2%       0.031s       4.81e-04s     65    24   Elemwise{Composite{tanh((i0 + i1))}}[(0, 0)](Dot22.0, InplaceDimShuffle{x,0}.0)
   0.9%    98.1%       0.004s       6.69e-05s     65    23   Dot22(Elemwise{ScalarSigmoid}[(0, 0)].0, W)
   0.8%    98.9%       0.004s       5.90e-05s     65    20   Elemwise{Add}[(0, 0)](CorrMM{valid, (2, 2), (1, 1)}.0, InplaceDimShuffle{x,0,x,x}.0)
   0.4%    99.3%       0.002s       2.96e-05s     65    30   Unique{return_index=False, return_inverse=False, return_counts=False}(Join.0)
   0.2%    99.5%       0.001s       1.63e-05s     65    35   dot(Elemwise{Composite{Cast{int64}(EQ(i0, i1))}}.0, Elemwise{Composite{Cast{int64}(EQ(i0, i1))}}.0)
   0.1%    99.6%       0.001s       7.72e-06s     65     8   Reshape{1}(<TensorType(float32, matrix)>, TensorConstant{(1,) of -1})
   0.1%    99.7%       0.000s       7.48e-06s     65    25   Dot22(Elemwise{Composite{tanh((i0 + i1))}}[(0, 0)].0, W)
   0.1%    99.8%       0.000s       5.83e-06s     65    27   Argmax(Elemwise{Add}[(0, 0)].0, TensorConstant{(1,) of 1})
   0.0%    99.8%       0.000s       3.09e-06s     65    28   Join(TensorConstant{0}, Elemwise{Cast{int32}}.0, argmax)
   0.0%    99.8%       0.000s       3.03e-06s     65    33   Elemwise{Composite{Cast{int64}(EQ(i0, i1))}}(InplaceDimShuffle{0,x}.0, InplaceDimShuffle{x,0}.0)
   0.0%    99.9%       0.000s       2.27e-06s     65    34   Elemwise{Composite{Cast{int64}(EQ(i0, i1))}}(InplaceDimShuffle{x,0}.0, InplaceDimShuffle{0,x}.0)
   0.0%    99.9%       0.000s       1.82e-06s     65    26   Elemwise{Add}[(0, 0)](Dot22.0, InplaceDimShuffle{x,0}.0)
   0.0%    99.9%       0.000s       7.63e-07s     65     3   Subtensor{::, ::, ::int64, ::int64}(<TensorType(float32, 4D)>, Constant{-1}, Constant{-1})
   0.0%    99.9%       0.000s       6.49e-07s     65     5   Subtensor{::, ::, ::int64, ::int64}(<TensorType(float32, 4D)>, Constant{-1}, Constant{-1})
   0.0%    99.9%       0.000s       6.20e-07s     65    21   Reshape{2}(Elemwise{Add}[(0, 0)].0, TensorConstant{[150  -1]})
   0.0%    99.9%       0.000s       6.13e-07s     65    12   Subtensor{int64:int64:}(Reshape{1}.0, ScalarFromTensor.0, ScalarFromTensor.0)
   ... (remaining 16 Apply instances account for 0.07%(0.00s) of the runtime)

Here are tips to potentially make your code run faster
                 (if you think of new ones, suggest them on the mailing list).
                 Test them first, as they are not guaranteed to always provide a speedup.
  - Try installing amdlibm and set the Theano flag lib.amdlibm=True. This speeds up only some Elemwise operation.
  - You have a dot operation that was not optimized to dot22 (which is faster). Make sure the inputs are float32 or float64, and are the same for both inputs. Currently they are: [TensorType(int64, matrix), TensorType(int64, matrix)]
Function profiling
==================
  Message: Sum of all(2) printed profiles at exit excluding Scan op profile.
  Time in 8064 calls to Function.__call__: 1.128081e+02s
  Time in Function.fn.__call__: 1.122575e+02s (99.512%)
  Time in thunks: 1.118720e+02s (99.170%)
  Total compile time: 1.212878e+00s
    Number of Apply nodes: 86
    Theano Optimizer time: 7.584071e-01s
       Theano validate time: 1.390290e-02s
    Theano Linker time (includes C, CUDA code generation/compiling): 8.260369e-02s
       Import time 2.029204e-02s
       Node make_thunk time 7.755518e-02s
           Node Elemwise{Composite{(i0 - (i1 * i2))}}[(0, 0)](<TensorType(float32, 4D)>, TensorConstant{(1, 1, 1, 1) of 0.3}, Subtensor{::, ::, ::int64, ::int64}.0) time 1.805305e-03s
           Node Elemwise{Composite{Switch(LT(Composite{Switch(GE(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(LT(i0, i1), (i0 + i2), i0)}(i0, i1, i2), i1, i3), i2), i1), i1, Composite{Switch(GE(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(LT(i0, i1), (i0 + i2), i0)}(i0, i1, i2), i1, i3), i2))}}[(0, 0)](Elemwise{Composite{(i0 * (i1 + i2))}}.0, TensorConstant{0}, Elemwise{Mul}[(0, 0)].0, TensorConstant{-1}) time 1.278877e-03s
           Node Elemwise{Composite{Switch(LT(Composite{Switch(GE(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), (i0 + i2), i0)}(i0, i1, i2), i1), i2), i1), i1, Composite{Switch(GE(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), (i0 + i2), i0)}(i0, i1, i2), i1), i2))}}[(0, 0)](Elemwise{mul,no_inplace}.0, TensorConstant{0}, Elemwise{Mul}[(0, 0)].0) time 1.208544e-03s
           Node Elemwise{Composite{(i0 * (i1 - sqr(i2)))}}[(0, 0)](Dot22.0, TensorConstant{(1, 1) of 1.0}, Elemwise{Composite{tanh((i0 + i1))}}[(0, 0)].0) time 1.158476e-03s
           Node InplaceDimShuffle{x,0}(b) time 1.150846e-03s

Time in all call to theano.grad() 1.684117e-02s
Time since theano import 115.861s
Class
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Class name>
  46.3%    46.3%      51.782s       2.80e-04s     C   184627      33   theano.tensor.elemwise.Elemwise
  19.7%    66.0%      22.052s       1.38e-03s     C    15998       2   theano.tensor.nnet.corr.CorrMM_gradWeights
  17.8%    83.8%      19.872s       1.23e-03s     C    16128       4   theano.tensor.nnet.corr.CorrMM
  11.8%    95.5%      13.186s       1.65e-03s     C     7999       1   theano.tensor.nnet.corr.CorrMM_gradInputs
   2.0%    97.6%       2.259s       7.06e-05s     C    31996       4   theano.tensor.elemwise.Sum
   1.1%    98.6%       1.187s       3.70e-05s     C    32126       6   theano.tensor.blas.Dot22
   0.4%    99.1%       0.474s       2.96e-05s     C    15998       2   theano.tensor.blas.Gemm
   0.3%    99.3%       0.284s       3.55e-05s     C     7999       1   theano.tensor.nnet.nnet.SoftmaxWithBias
   0.3%    99.6%       0.281s       3.48e-05s     Py    8064       2   theano.tensor.extra_ops.Unique
   0.1%    99.7%       0.142s       1.76e-05s     Py    8064       2   theano.tensor.basic.Dot
   0.1%    99.8%       0.108s       3.37e-06s     PyC    32191       7   theano.tensor.basic.Reshape
   0.1%    99.8%       0.056s       4.67e-07s     C   120570      24   theano.tensor.elemwise.DimShuffle
   0.0%    99.9%       0.054s       6.67e-06s     C     8064       2   theano.tensor.basic.Argmax
   0.0%    99.9%       0.053s       5.55e-07s     C    96248      16   theano.tensor.subtensor.Subtensor
   0.0%   100.0%       0.030s       3.77e-06s     C     8064       2   theano.tensor.basic.Join
   0.0%   100.0%       0.019s       2.37e-06s     C     7999       1   theano.tensor.nnet.nnet.CrossentropySoftmax1HotWithBiasDx
   0.0%   100.0%       0.018s       3.81e-07s     C    47994       6   theano.compile.ops.Shape_i
   0.0%   100.0%       0.009s       5.36e-07s     C    15998       2   theano.tensor.opt.MakeVector
   0.0%   100.0%       0.003s       1.75e-07s     C    16128       4   theano.tensor.basic.ScalarFromTensor
   0.0%   100.0%       0.002s       3.00e-07s     C     7999       1   theano.compile.ops.Rebroadcast
   ... (remaining 0 Classes account for   0.00%(0.00s) of the runtime)

Ops
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Op name>
  25.1%    25.1%      28.046s       3.48e-03s     C     8064        2   Elemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)]
  19.7%    44.8%      22.052s       1.38e-03s     C     15998        2   CorrMM_gradWeights{valid, (2, 2), (1, 1)}
  17.8%    62.5%      19.872s       1.23e-03s     C     16128        4   CorrMM{valid, (2, 2), (1, 1)}
  11.8%    74.3%      13.186s       1.65e-03s     C     7999        1   CorrMM_gradInputs{valid, (2, 2), (1, 1)}
   8.7%    83.0%       9.704s       1.21e-03s     C     7999        1   Elemwise{Composite{(i0 * scalar_sigmoid(i1) * (i2 - scalar_sigmoid(i1)))}}[(0, 0)]
   8.2%    91.2%       9.153s       1.14e-03s     C     7999        1   sigmoid
   3.4%    94.5%       3.754s       4.65e-04s     C     8064        2   Elemwise{Composite{tanh((i0 + i1))}}[(0, 0)]
   1.9%    96.4%       2.114s       1.32e-04s     C     15998        2   Sum{axis=[0, 2, 3], acc_dtype=float64}
   1.1%    97.5%       1.187s       3.70e-05s     C     32126        6   Dot22
   0.4%    97.9%       0.493s       3.06e-05s     C     16128        4   Elemwise{Add}[(0, 0)]
   0.4%    98.4%       0.474s       2.96e-05s     C     15998        2   Gemm{inplace}
   0.3%    98.7%       0.367s       4.59e-05s     C     7999        1   Elemwise{Composite{(i0 * i1 * (i2 - i1))}}[(0, 0)]
   0.3%    98.9%       0.284s       3.55e-05s     C     7999        1   SoftmaxWithBias
   0.3%    99.2%       0.281s       3.48e-05s     Py    8064        2   Unique{return_index=False, return_inverse=False, return_counts=False}
   0.1%    99.3%       0.145s       9.06e-06s     C     15998        2   Sum{axis=[0], acc_dtype=float64}
   0.1%    99.4%       0.142s       1.76e-05s     Py    8064        2   dot
   0.1%    99.5%       0.089s       1.11e-05s     Py    8064        2   Reshape{1}
   0.1%    99.6%       0.072s       1.11e-03s     C       65        1   Elemwise{ScalarSigmoid}[(0, 0)]
   0.1%    99.6%       0.056s       1.18e-06s     C     47994        6   Elemwise{Composite{(i0 - (i1 * i2))}}[(0, 0)]
   0.0%    99.7%       0.055s       6.92e-06s     C     7999        1   Elemwise{Composite{(i0 * (i1 - sqr(i2)))}}[(0, 0)]
   ... (remaining 30 Ops account for   0.31%(0.35s) of the runtime)

Apply
------
<% time> <sum %> <apply time> <time per call> <#call> <id> <Apply name>
  24.9%    24.9%      27.830s       3.48e-03s   7999    40   Elemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)](CorrMM{valid, (2, 2), (1, 1)}.0, InplaceDimShuffle{x,0,x,x}.0)
  11.8%    36.7%      13.186s       1.65e-03s   7999    74   CorrMM_gradInputs{valid, (2, 2), (1, 1)}(Subtensor{::, ::, ::int64, ::int64}.0, Elemwise{Composite{(i0 * scalar_sigmoid(i1) * (i2 - scalar_sigmoid(i1)))}}[(0, 0)].0, Subtensor{int64}.0, Subtensor{int64}.0)
  10.3%    47.0%      11.548s       1.44e-03s   7999    75   CorrMM_gradWeights{valid, (2, 2), (1, 1)}(Elemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)].0, Elemwise{Composite{(i0 * scalar_sigmoid(i1) * (i2 - scalar_sigmoid(i1)))}}[(0, 0)].0, Subtensor{int64}.0, Subtensor{int64}.0)
  10.0%    57.0%      11.175s       1.40e-03s   7999    41   CorrMM{valid, (2, 2), (1, 1)}(Elemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)].0, Subtensor{::, ::, ::int64, ::int64}.0)
   9.4%    66.4%      10.504s       1.31e-03s   7999    80   CorrMM_gradWeights{valid, (2, 2), (1, 1)}(InplaceDimShuffle{0,x,1,2}.0, Elemwise{Composite{(i0 * i1 * (i2 - i1))}}[(0, 0)].0, Subtensor{int64}.0, Subtensor{int64}.0)
   8.7%    75.0%       9.704s       1.21e-03s   7999    71   Elemwise{Composite{(i0 * scalar_sigmoid(i1) * (i2 - scalar_sigmoid(i1)))}}[(0, 0)](Reshape{4}.0, Elemwise{Add}[(0, 0)].0, TensorConstant{(1, 1, 1, 1) of 1.0})
   8.2%    83.2%       9.153s       1.14e-03s   7999    44   sigmoid(Reshape{2}.0)
   7.6%    90.9%       8.537s       1.07e-03s   7999    39   CorrMM{valid, (2, 2), (1, 1)}(InplaceDimShuffle{0,x,1,2}.0, Subtensor{::, ::, ::int64, ::int64}.0)
   3.3%    94.2%       3.722s       4.65e-04s   7999    47   Elemwise{Composite{tanh((i0 + i1))}}[(0, 0)](Dot22.0, InplaceDimShuffle{x,0}.0)
   1.3%    95.5%       1.492s       1.86e-04s   7999    79   Sum{axis=[0, 2, 3], acc_dtype=float64}(Elemwise{Composite{(i0 * i1 * (i2 - i1))}}[(0, 0)].0)
   0.6%    96.1%       0.622s       7.78e-05s   7999    73   Sum{axis=[0, 2, 3], acc_dtype=float64}(Elemwise{Composite{(i0 * scalar_sigmoid(i1) * (i2 - scalar_sigmoid(i1)))}}[(0, 0)].0)
   0.5%    96.6%       0.587s       7.34e-05s   7999    45   Dot22(sigmoid.0, W)
   0.4%    97.0%       0.473s       5.91e-05s   7999    42   Elemwise{Add}[(0, 0)](CorrMM{valid, (2, 2), (1, 1)}.0, InplaceDimShuffle{x,0,x,x}.0)
   0.4%    97.4%       0.469s       5.86e-05s   7999    63   Dot22(Elemwise{Composite{(i0 * (i1 - sqr(i2)))}}[(0, 0)].0, W.T)
   0.4%    97.8%       0.422s       5.28e-05s   7999    66   Gemm{inplace}(W, TensorConstant{-0.30000001192092896}, InplaceDimShuffle{1,0}.0, Elemwise{Composite{(i0 * (i1 - sqr(i2)))}}[(0, 0)].0, TensorConstant{1.0})
   0.3%    98.1%       0.367s       4.59e-05s   7999    77   Elemwise{Composite{(i0 * i1 * (i2 - i1))}}[(0, 0)](CorrMM_gradInputs{valid, (2, 2), (1, 1)}.0, Elemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)].0, TensorConstant{(1, 1, 1, 1) of 1.0})
   0.3%    98.4%       0.284s       3.55e-05s   7999    50   SoftmaxWithBias(Dot22.0, b)
   0.2%    98.6%       0.279s       3.48e-05s   7999    61   Unique{return_index=False, return_inverse=False, return_counts=False}(Join.0)
   0.2%    98.8%       0.216s       3.32e-03s     65    18   Elemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)](CorrMM{valid, (2, 2), (1, 1)}.0, InplaceDimShuffle{x,0,x,x}.0)
   0.1%    99.0%       0.141s       1.76e-05s   7999    72   dot(Elemwise{Composite{Cast{int64}(EQ(i0, i1))}}.0, Elemwise{Composite{Cast{int64}(EQ(i0, i1))}}.0)
   ... (remaining 102 Apply instances account for 1.04%(1.16s) of the runtime)

Here are tips to potentially make your code run faster
                 (if you think of new ones, suggest them on the mailing list).
                 Test them first, as they are not guaranteed to always provide a speedup.
  - Try installing amdlibm and set the Theano flag lib.amdlibm=True. This speeds up only some Elemwise operation.
  - You have a dot operation that was not optimized to dot22 (which is faster). Make sure the inputs are float32 or float64, and are the same for both inputs. Currently they are: [TensorType(int64, matrix), TensorType(int64, matrix)]
  - You have a dot operation that was not optimized to dot22 (which is faster). Make sure the inputs are float32 or float64, and are the same for both inputs. Currently they are: [TensorType(int64, matrix), TensorType(int64, matrix)]
