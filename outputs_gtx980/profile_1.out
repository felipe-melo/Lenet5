Using cuDNN version 5110 on context None
Mapped name None to device cuda0: GeForce GTX 980 (0000:01:00.0)
/opt/anaconda3/lib/python3.6/site-packages/matplotlib/font_manager.py:280: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.
  'Matplotlib is building the font cache using fc-list. '
Function profiling
==================
  Message: /home/hal/Documentos/Felipe/Lenet5/lenet_5/Lenet5.py:92
  Time in 399 calls to Function.__call__: 3.047571e-01s
  Time in Function.fn.__call__: 2.856042e-01s (93.715%)
  Time in thunks: 2.564952e-01s (84.164%)
  Total compile time: 4.193205e+01s
    Number of Apply nodes: 164
    Theano Optimizer time: 5.179054e+00s
       Theano validate time: 2.794552e-02s
    Theano Linker time (includes C, CUDA code generation/compiling): 3.673674e+01s
       Import time 5.615091e-02s
       Node make_thunk time 3.672769e+01s
           Node GpuElemwise{Composite{(i0 * scalar_sigmoid(i1) * (i2 - scalar_sigmoid(i1)))}}[(0, 0)]<gpuarray>(GpuReshape{4}.0, GpuElemwise{Add}[(0, 0)]<gpuarray>.0, GpuArrayConstant{[[[[ 1.]]]]}) time 2.090707e+00s
           Node GpuElemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)]<gpuarray>(GpuDnnConv{algo='small', inplace=True}.0, InplaceGpuDimShuffle{x,0,x,x}.0) time 2.077197e+00s
           Node GpuElemwise{Composite{(i0 * i1 * (i2 - i1))}}[(0, 0)]<gpuarray>(GpuDnnConvGradI{algo='none', inplace=True}.0, GpuElemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)]<gpuarray>.0, GpuArrayConstant{[[[[ 1.]]]]}) time 2.052404e+00s
           Node GpuElemwise{Add}[(0, 0)]<gpuarray>(GpuDnnConv{algo='small', inplace=True}.0, InplaceGpuDimShuffle{x,0,x,x}.0) time 2.035356e+00s
           Node GpuElemwise{Sub}[(0, 0)]<gpuarray>(<GpuArrayType<None>(float32, (False, False, False, False))>, GpuDnnConvGradW{algo='none', inplace=True}.0) time 2.012924e+00s

Time in all call to theano.grad() 1.813912e-02s
Time since theano import 66.300s
Class
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Class name>
  44.5%    44.5%       0.114s       9.54e-05s     C     1197       3   theano.gpuarray.basic_ops.HostFromGpu
  13.7%    58.2%       0.035s       4.39e-06s     C     7980      20   theano.gpuarray.elemwise.GpuElemwise
   5.0%    63.2%       0.013s       1.61e-05s     C      798       2   theano.gpuarray.dnn.GpuDnnConvGradW
   4.6%    67.8%       0.012s       2.99e-05s     Py     399       1   theano.tensor.extra_ops.Unique
   4.1%    71.9%       0.011s       4.39e-06s     C     2394       6   theano.gpuarray.basic_ops.GpuFromHost
   3.8%    75.8%       0.010s       6.18e-06s     C     1596       4   theano.gpuarray.blas.GpuDot22
   3.8%    79.5%       0.010s       1.21e-05s     C      798       2   theano.gpuarray.dnn.GpuDnnConv
   2.8%    82.4%       0.007s       1.83e-05s     C      399       1   theano.gpuarray.dnn.GpuDnnConvGradI
   2.7%    85.1%       0.007s       1.76e-05s     Py     399       1   theano.tensor.basic.Dot
   2.2%    87.3%       0.006s       1.11e-06s     C     5187      13   theano.gpuarray.elemwise.GpuDimShuffle
   2.2%    89.5%       0.006s       3.49e-06s     C     1596       4   theano.gpuarray.elemwise.GpuCAReduceCuda
   1.7%    91.2%       0.004s       5.51e-06s     C      798       2   theano.gpuarray.blas.GpuGemm
   1.3%    92.6%       0.003s       8.58e-06s     C      399       1   theano.gpuarray.basic_ops.GpuJoin
   0.9%    93.5%       0.002s       3.94e-07s     C     5985      15   theano.tensor.elemwise.Elemwise
   0.9%    94.4%       0.002s       5.59e-06s     C      399       1   theano.tensor.basic.Argmax
   0.8%    95.2%       0.002s       1.36e-06s     C     1596       4   theano.gpuarray.basic_ops.GpuReshape
   0.8%    96.0%       0.002s       9.95e-07s     C     1995       5   theano.gpuarray.basic_ops.GpuAllocEmpty
   0.7%    96.6%       0.002s       2.35e-07s     C     7182      18   theano.compile.ops.Shape_i
   0.6%    97.3%       0.002s       1.86e-07s     C     8778      22   theano.tensor.subtensor.Subtensor
   0.5%    97.8%       0.001s       3.46e-06s     C      399       1   theano.gpuarray.nnet.GpuCrossentropySoftmax1HotWithBiasDx
   ... (remaining 10 Classes account for   2.19%(0.01s) of the runtime)

Ops
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Op name>
  44.5%    44.5%       0.114s       9.54e-05s     C     1197        3   HostFromGpu(gpuarray)
   5.0%    49.5%       0.013s       1.61e-05s     C      798        2   GpuDnnConvGradW{algo='none', inplace=True}
   4.6%    54.2%       0.012s       2.99e-05s     Py     399        1   Unique{return_index=False, return_inverse=False, return_counts=False}
   4.1%    58.3%       0.011s       4.39e-06s     C     2394        6   GpuFromHost<None>
   3.8%    62.1%       0.010s       6.18e-06s     C     1596        4   GpuDot22
   3.8%    65.9%       0.010s       1.21e-05s     C      798        2   GpuDnnConv{algo='small', inplace=True}
   2.8%    68.7%       0.007s       1.83e-05s     C      399        1   GpuDnnConvGradI{algo='none', inplace=True}
   2.8%    71.5%       0.007s       3.57e-06s     C     1995        5   GpuElemwise{Composite{(i0 - (i1 * i2))}}[(0, 0)]<gpuarray>
   2.7%    74.2%       0.007s       1.76e-05s     Py     399        1   dot
   1.7%    75.9%       0.004s       5.51e-06s     C      798        2   GpuGemm{inplace=True}
   1.4%    77.3%       0.004s       8.99e-06s     C      399        1   GpuElemwise{Cast{int32}}[]<gpuarray>
   1.3%    78.7%       0.003s       8.58e-06s     C      399        1   GpuJoin
   1.2%    79.9%       0.003s       3.88e-06s     C      798        2   GpuCAReduceCuda{add}{0, 2, 3}
   1.2%    81.1%       0.003s       3.86e-06s     C      798        2   GpuElemwise{Add}[(0, 0)]<gpuarray>
   1.2%    82.2%       0.003s       7.44e-06s     C      399        1   GpuElemwise{Composite{Cast{int64}(EQ(i0, i1))}}[]<gpuarray>
   1.0%    83.2%       0.002s       3.09e-06s     C      798        2   GpuCAReduceCuda{add}{0}
   0.9%    84.1%       0.002s       5.62e-06s     C      399        1   GpuElemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)]<gpuarray>
   0.9%    84.9%       0.002s       5.59e-06s     C      399        1   Argmax
   0.8%    85.7%       0.002s       9.95e-07s     C     1995        5   GpuAllocEmpty{dtype='float32', context_name=None}
   0.8%    86.5%       0.002s       4.90e-06s     C      399        1   GpuElemwise{Composite{(i0 * i1 * (i2 - i1))}}[(0, 0)]<gpuarray>
   ... (remaining 45 Ops account for  13.52%(0.03s) of the runtime)

Apply
------
<% time> <sum %> <apply time> <time per call> <#call> <id> <Apply name>
  41.1%    41.1%       0.105s       2.64e-04s    399   101   HostFromGpu(gpuarray)(GpuElemwise{Add}[(0, 0)]<gpuarray>.0)
   4.6%    45.7%       0.012s       2.99e-05s    399   118   Unique{return_index=False, return_inverse=False, return_counts=False}(HostFromGpu(gpuarray).0)
   2.8%    48.6%       0.007s       1.83e-05s    399   145   GpuDnnConvGradI{algo='none', inplace=True}(GpuContiguous.0, GpuContiguous.0, GpuAllocEmpty{dtype='float32', context_name=None}.0, GpuDnnConvDesc{border_mode='valid', subsample=(2, 2), conv_mode='conv', precision='float32'}.0, Constant{1.0}, Constant{0.0})
   2.7%    51.3%       0.007s       1.76e-05s    399   144   dot(HostFromGpu(gpuarray).0, Elemwise{Composite{Cast{int64}(EQ(i0, i1))}}.0)
   2.7%    54.0%       0.007s       1.74e-05s    399   161   GpuDnnConvGradW{algo='none', inplace=True}(GpuContiguous.0, GpuContiguous.0, GpuAllocEmpty{dtype='float32', context_name=None}.0, GpuDnnConvDesc{border_mode='valid', subsample=(2, 2), conv_mode='conv', precision='float32'}.0, Constant{1.0}, Constant{0.0})
   2.3%    56.3%       0.006s       1.48e-05s    399   146   GpuDnnConvGradW{algo='none', inplace=True}(GpuContiguous.0, GpuContiguous.0, GpuAllocEmpty{dtype='float32', context_name=None}.0, GpuDnnConvDesc{border_mode='valid', subsample=(2, 2), conv_mode='conv', precision='float32'}.0, Constant{0.3}, Constant{0.0})
   2.2%    58.6%       0.006s       1.43e-05s    399    72   GpuDnnConv{algo='small', inplace=True}(GpuContiguous.0, GpuContiguous.0, GpuAllocEmpty{dtype='float32', context_name=None}.0, GpuDnnConvDesc{border_mode='valid', subsample=(2, 2), conv_mode='conv', precision='float32'}.0, Constant{1.0}, Constant{0.0})
   2.0%    60.5%       0.005s       1.26e-05s    399   141   HostFromGpu(gpuarray)(GpuElemwise{Composite{Cast{int64}(EQ(i0, i1))}}[]<gpuarray>.0)
   1.5%    62.1%       0.004s       9.84e-06s    399    89   GpuDnnConv{algo='small', inplace=True}(GpuContiguous.0, GpuContiguous.0, GpuAllocEmpty{dtype='float32', context_name=None}.0, GpuDnnConvDesc{border_mode='valid', subsample=(2, 2), conv_mode='conv', precision='float32'}.0, Constant{1.0}, Constant{0.0})
   1.4%    63.5%       0.004s       9.27e-06s    399   116   HostFromGpu(gpuarray)(GpuJoin.0)
   1.4%    64.9%       0.004s       8.99e-06s    399    49   GpuElemwise{Cast{int32}}[]<gpuarray>(GpuSubtensor{int64:int64:}.0)
   1.3%    66.2%       0.003s       8.58e-06s    399   112   GpuJoin(TensorConstant{0}, GpuElemwise{Cast{int32}}[]<gpuarray>.0, GpuFromHost<None>.0)
   1.2%    67.4%       0.003s       7.44e-06s    399   131   GpuElemwise{Composite{Cast{int64}(EQ(i0, i1))}}[]<gpuarray>(InplaceGpuDimShuffle{x,0}.0, InplaceGpuDimShuffle{0,x}.0)
   1.1%    68.5%       0.003s       7.30e-06s    399   103   GpuDot22(GpuCrossentropySoftmax1HotWithBiasDx.0, InplaceGpuDimShuffle{1,0}.0)
   1.1%    69.6%       0.003s       7.18e-06s    399    93   GpuDot22(Gpusigmoid.0, W)
   1.0%    70.7%       0.003s       6.51e-06s    399   123   GpuFromHost<None>(Unique{return_index=False, return_inverse=False, return_counts=False}.0)
   1.0%    71.7%       0.003s       6.38e-06s    399   113   GpuGemm{inplace=True}(W, TensorConstant{-0.30000001192092896}, InplaceGpuDimShuffle{1,0}.0, GpuElemwise{Composite{(i0 * (i1 - sqr(i2)))}}[(0, 0)]<gpuarray>.0, TensorConstant{1.0})
   0.9%    72.6%       0.002s       5.95e-06s    399   108   GpuFromHost<None>(argmax)
   0.9%    73.5%       0.002s       5.62e-06s    399    73   GpuElemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)]<gpuarray>(GpuDnnConv{algo='small', inplace=True}.0, InplaceGpuDimShuffle{x,0,x,x}.0)
   0.9%    74.3%       0.002s       5.59e-06s    399   104   Argmax(HostFromGpu(gpuarray).0, TensorConstant{(1,) of 1})
   ... (remaining 144 Apply instances account for 25.68%(0.07s) of the runtime)

Here are tips to potentially make your code run faster
                 (if you think of new ones, suggest them on the mailing list).
                 Test them first, as they are not guaranteed to always provide a speedup.
  - Try installing amdlibm and set the Theano flag lib.amdlibm=True. This speeds up only some Elemwise operation.
  - You have a dot operation that was not optimized to dot22 (which is faster). Make sure the inputs are float32 or float64, and are the same for both inputs. Currently they are: [TensorType(int64, matrix), TensorType(int64, matrix)]
Function profiling
==================
  Message: /home/hal/Documentos/Felipe/Lenet5/lenet_5/Lenet5.py:111
  Time in 65 calls to Function.__call__: 2.352643e-02s
  Time in Function.fn.__call__: 2.076340e-02s (88.256%)
  Time in thunks: 1.881313e-02s (79.966%)
  Total compile time: 1.715875e+00s
    Number of Apply nodes: 81
    Theano Optimizer time: 1.926312e-01s
       Theano validate time: 5.242348e-03s
    Theano Linker time (includes C, CUDA code generation/compiling): 1.513514e+00s
       Import time 1.918077e-03s
       Node make_thunk time 1.510380e+00s
           Node GpuElemwise{ScalarSigmoid}[(0, 0)]<gpuarray>(GpuReshape{2}.0) time 1.225191e+00s
           Node InplaceGpuDimShuffle{x,0,x,x}(<GpuArrayType<None>(float32, (False,))>) time 2.445204e-01s
           Node Shape_i{2}(GpuContiguous.0) time 1.573563e-03s
           Node InplaceGpuDimShuffle{x,0}(b) time 1.547575e-03s
           Node Elemwise{Composite{(((i0 - (((i1 - i2) * i3) + i4)) // i5) + i6)}}[(0, 0)](Shape_i{3}.0, Shape_i{3}.0, TensorConstant{1}, TensorConstant{1}, TensorConstant{1}, TensorConstant{2}, TensorConstant{1}) time 8.416176e-04s

Time in all call to theano.grad() 1.813912e-02s
Time since theano import 66.316s
Class
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Class name>
  38.9%    38.9%       0.007s       3.76e-05s     C      195       3   theano.gpuarray.basic_ops.HostFromGpu
  12.9%    51.9%       0.002s       5.35e-06s     C      455       7   theano.gpuarray.elemwise.GpuElemwise
  10.1%    62.0%       0.002s       2.94e-05s     Py      65       1   theano.tensor.extra_ops.Unique
   8.2%    70.3%       0.002s       1.19e-05s     C      130       2   theano.gpuarray.dnn.GpuDnnConv
   6.1%    76.4%       0.001s       1.76e-05s     Py      65       1   theano.tensor.basic.Dot
   4.5%    80.8%       0.001s       6.46e-06s     C      130       2   theano.gpuarray.blas.GpuDot22
   4.3%    85.2%       0.001s       6.28e-06s     C      130       2   theano.gpuarray.basic_ops.GpuFromHost
   2.9%    88.1%       0.001s       8.40e-06s     C       65       1   theano.gpuarray.basic_ops.GpuJoin
   2.9%    91.0%       0.001s       1.19e-06s     C      455       7   theano.gpuarray.elemwise.GpuDimShuffle
   2.0%    93.0%       0.000s       5.76e-06s     C       65       1   theano.tensor.basic.Argmax
   2.0%    94.9%       0.000s       3.78e-07s     C      975      15   theano.tensor.elemwise.Elemwise
   1.7%    96.6%       0.000s       1.61e-06s     C      195       3   theano.gpuarray.basic_ops.GpuReshape
   0.7%    97.3%       0.000s       1.05e-06s     C      130       2   theano.gpuarray.basic_ops.GpuAllocEmpty
   0.7%    98.0%       0.000s       1.62e-07s     C      780      12   theano.compile.ops.Shape_i
   0.6%    98.6%       0.000s       9.37e-07s     C      130       2   theano.gpuarray.subtensor.GpuSubtensor
   0.3%    99.0%       0.000s       4.84e-07s     C      130       2   theano.gpuarray.dnn.GpuDnnConvDesc
   0.3%    99.2%       0.000s       4.07e-07s     C      130       2   theano.tensor.elemwise.DimShuffle
   0.2%    99.5%       0.000s       3.25e-07s     C      130       2   theano.compile.ops.Shape
   0.2%    99.7%       0.000s       7.70e-08s     C      520       8   theano.tensor.opt.Assert
   0.2%    99.9%       0.000s       1.27e-07s     C      260       4   theano.gpuarray.basic_ops.GpuContiguous
   ... (remaining 1 Classes account for   0.15%(0.00s) of the runtime)

Ops
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Op name>
  38.9%    38.9%       0.007s       3.76e-05s     C      195        3   HostFromGpu(gpuarray)
  10.1%    49.1%       0.002s       2.94e-05s     Py      65        1   Unique{return_index=False, return_inverse=False, return_counts=False}
   8.2%    57.3%       0.002s       1.19e-05s     C      130        2   GpuDnnConv{algo='small', inplace=True}
   6.1%    63.4%       0.001s       1.76e-05s     Py      65        1   dot
   4.5%    67.9%       0.001s       6.46e-06s     C      130        2   GpuDot22
   4.3%    72.2%       0.001s       6.28e-06s     C      130        2   GpuFromHost<None>
   3.0%    75.2%       0.001s       8.65e-06s     C       65        1   GpuElemwise{Cast{int32}}[]<gpuarray>
   2.9%    78.1%       0.001s       8.40e-06s     C       65        1   GpuJoin
   2.9%    81.0%       0.001s       4.13e-06s     C      130        2   GpuElemwise{Add}[(0, 0)]<gpuarray>
   2.5%    83.5%       0.000s       7.37e-06s     C       65        1   GpuElemwise{Composite{Cast{int64}(EQ(i0, i1))}}[]<gpuarray>
   2.0%    85.6%       0.000s       5.90e-06s     C       65        1   GpuElemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)]<gpuarray>
   2.0%    87.6%       0.000s       5.76e-06s     C       65        1   Argmax
   1.4%    88.9%       0.000s       4.04e-06s     C       65        1   GpuElemwise{Composite{tanh((i0 + i1))}}[(0, 0)]<gpuarray>
   1.1%    90.1%       0.000s       3.26e-06s     C       65        1   GpuElemwise{ScalarSigmoid}[(0, 0)]<gpuarray>
   1.1%    91.2%       0.000s       1.07e-06s     C      195        3   InplaceGpuDimShuffle{x,0}
   1.0%    92.1%       0.000s       2.78e-06s     C       65        1   Elemwise{Composite{Cast{int64}(EQ(i0, i1))}}
   1.0%    93.1%       0.000s       1.38e-06s     C      130        2   InplaceGpuDimShuffle{x,0,x,x}
   0.9%    94.0%       0.000s       2.53e-06s     C       65        1   GpuReshape{1}
   0.7%    94.7%       0.000s       1.05e-06s     C      130        2   GpuAllocEmpty{dtype='float32', context_name=None}
   0.6%    95.3%       0.000s       9.37e-07s     C      130        2   GpuSubtensor{int64:int64:}
   ... (remaining 22 Ops account for   4.66%(0.00s) of the runtime)

Apply
------
<% time> <sum %> <apply time> <time per call> <#call> <id> <Apply name>
  31.4%    31.4%       0.006s       9.08e-05s     65    67   HostFromGpu(gpuarray)(GpuElemwise{Add}[(0, 0)]<gpuarray>.0)
  10.1%    41.5%       0.002s       2.94e-05s     65    73   Unique{return_index=False, return_inverse=False, return_counts=False}(HostFromGpu(gpuarray).0)
   6.1%    47.6%       0.001s       1.76e-05s     65    80   dot(HostFromGpu(gpuarray).0, Elemwise{Composite{Cast{int64}(EQ(i0, i1))}}.0)
   4.7%    52.3%       0.001s       1.36e-05s     65    44   GpuDnnConv{algo='small', inplace=True}(GpuContiguous.0, GpuContiguous.0, GpuAllocEmpty{dtype='float32', context_name=None}.0, GpuDnnConvDesc{border_mode='valid', subsample=(2, 2), conv_mode='conv', precision='float32'}.0, Constant{1.0}, Constant{0.0})
   4.4%    56.7%       0.001s       1.26e-05s     65    79   HostFromGpu(gpuarray)(GpuElemwise{Composite{Cast{int64}(EQ(i0, i1))}}[]<gpuarray>.0)
   3.6%    60.2%       0.001s       1.03e-05s     65    59   GpuDnnConv{algo='small', inplace=True}(GpuContiguous.0, GpuContiguous.0, GpuAllocEmpty{dtype='float32', context_name=None}.0, GpuDnnConvDesc{border_mode='valid', subsample=(2, 2), conv_mode='conv', precision='float32'}.0, Constant{1.0}, Constant{0.0})
   3.2%    63.4%       0.001s       9.26e-06s     65    72   HostFromGpu(gpuarray)(GpuJoin.0)
   3.0%    66.4%       0.001s       8.65e-06s     65    28   GpuElemwise{Cast{int32}}[]<gpuarray>(GpuSubtensor{int64:int64:}.0)
   2.9%    69.3%       0.001s       8.40e-06s     65    71   GpuJoin(TensorConstant{0}, GpuElemwise{Cast{int32}}[]<gpuarray>.0, GpuFromHost<None>.0)
   2.6%    72.0%       0.000s       7.64e-06s     65    63   GpuDot22(GpuElemwise{ScalarSigmoid}[(0, 0)]<gpuarray>.0, W)
   2.5%    74.5%       0.000s       7.37e-06s     65    78   GpuElemwise{Composite{Cast{int64}(EQ(i0, i1))}}[]<gpuarray>(InplaceGpuDimShuffle{x,0}.0, InplaceGpuDimShuffle{0,x}.0)
   2.3%    76.8%       0.000s       6.64e-06s     65    75   GpuFromHost<None>(Unique{return_index=False, return_inverse=False, return_counts=False}.0)
   2.0%    78.8%       0.000s       5.92e-06s     65    69   GpuFromHost<None>(argmax)
   2.0%    80.9%       0.000s       5.90e-06s     65    45   GpuElemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)]<gpuarray>(GpuDnnConv{algo='small', inplace=True}.0, InplaceGpuDimShuffle{x,0,x,x}.0)
   2.0%    82.9%       0.000s       5.76e-06s     65    68   Argmax(HostFromGpu(gpuarray).0, TensorConstant{(1,) of 1})
   1.8%    84.7%       0.000s       5.29e-06s     65    65   GpuDot22(GpuElemwise{Composite{tanh((i0 + i1))}}[(0, 0)]<gpuarray>.0, W)
   1.6%    86.3%       0.000s       4.69e-06s     65    60   GpuElemwise{Add}[(0, 0)]<gpuarray>(GpuDnnConv{algo='small', inplace=True}.0, InplaceGpuDimShuffle{x,0,x,x}.0)
   1.4%    87.7%       0.000s       4.04e-06s     65    64   GpuElemwise{Composite{tanh((i0 + i1))}}[(0, 0)]<gpuarray>(GpuDot22.0, InplaceGpuDimShuffle{x,0}.0)
   1.2%    88.9%       0.000s       3.57e-06s     65    66   GpuElemwise{Add}[(0, 0)]<gpuarray>(GpuDot22.0, InplaceGpuDimShuffle{x,0}.0)
   1.1%    90.1%       0.000s       3.26e-06s     65    62   GpuElemwise{ScalarSigmoid}[(0, 0)]<gpuarray>(GpuReshape{2}.0)
   ... (remaining 61 Apply instances account for 9.93%(0.00s) of the runtime)

Here are tips to potentially make your code run faster
                 (if you think of new ones, suggest them on the mailing list).
                 Test them first, as they are not guaranteed to always provide a speedup.
  - Try installing amdlibm and set the Theano flag lib.amdlibm=True. This speeds up only some Elemwise operation.
  - You have a dot operation that was not optimized to dot22 (which is faster). Make sure the inputs are float32 or float64, and are the same for both inputs. Currently they are: [TensorType(int64, matrix), TensorType(int64, matrix)]
Function profiling
==================
  Message: Sum of all(2) printed profiles at exit excluding Scan op profile.
  Time in 464 calls to Function.__call__: 3.282835e-01s
  Time in Function.fn.__call__: 3.063676e-01s (93.324%)
  Time in thunks: 2.753084e-01s (83.863%)
  Total compile time: 4.364793e+01s
    Number of Apply nodes: 164
    Theano Optimizer time: 5.371686e+00s
       Theano validate time: 3.318787e-02s
    Theano Linker time (includes C, CUDA code generation/compiling): 3.825025e+01s
       Import time 5.806899e-02s
       Node make_thunk time 3.823807e+01s
           Node GpuElemwise{Composite{(i0 * scalar_sigmoid(i1) * (i2 - scalar_sigmoid(i1)))}}[(0, 0)]<gpuarray>(GpuReshape{4}.0, GpuElemwise{Add}[(0, 0)]<gpuarray>.0, GpuArrayConstant{[[[[ 1.]]]]}) time 2.090707e+00s
           Node GpuElemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)]<gpuarray>(GpuDnnConv{algo='small', inplace=True}.0, InplaceGpuDimShuffle{x,0,x,x}.0) time 2.077197e+00s
           Node GpuElemwise{Composite{(i0 * i1 * (i2 - i1))}}[(0, 0)]<gpuarray>(GpuDnnConvGradI{algo='none', inplace=True}.0, GpuElemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)]<gpuarray>.0, GpuArrayConstant{[[[[ 1.]]]]}) time 2.052404e+00s
           Node GpuElemwise{Add}[(0, 0)]<gpuarray>(GpuDnnConv{algo='small', inplace=True}.0, InplaceGpuDimShuffle{x,0,x,x}.0) time 2.035356e+00s
           Node GpuElemwise{Sub}[(0, 0)]<gpuarray>(<GpuArrayType<None>(float32, (False, False, False, False))>, GpuDnnConvGradW{algo='none', inplace=True}.0) time 2.012924e+00s

Time in all call to theano.grad() 1.813912e-02s
Time since theano import 66.324s
Class
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Class name>
  44.1%    44.1%       0.121s       8.73e-05s     C     1392       6   theano.gpuarray.basic_ops.HostFromGpu
  13.6%    57.7%       0.037s       4.44e-06s     C     8435      27   theano.gpuarray.elemwise.GpuElemwise
   5.0%    62.8%       0.014s       2.98e-05s     Py     464       2   theano.tensor.extra_ops.Unique
   4.7%    67.4%       0.013s       1.61e-05s     C      798       2   theano.gpuarray.dnn.GpuDnnConvGradW
   4.1%    71.6%       0.011s       4.49e-06s     C     2524       8   theano.gpuarray.basic_ops.GpuFromHost
   4.1%    75.6%       0.011s       1.21e-05s     C      928       4   theano.gpuarray.dnn.GpuDnnConv
   3.9%    79.5%       0.011s       6.20e-06s     C     1726       6   theano.gpuarray.blas.GpuDot22
   3.0%    82.5%       0.008s       1.76e-05s     Py     464       2   theano.tensor.basic.Dot
   2.7%    85.1%       0.007s       1.83e-05s     C      399       1   theano.gpuarray.dnn.GpuDnnConvGradI
   2.3%    87.4%       0.006s       1.11e-06s     C     5642      20   theano.gpuarray.elemwise.GpuDimShuffle
   2.0%    89.4%       0.006s       3.49e-06s     C     1596       4   theano.gpuarray.elemwise.GpuCAReduceCuda
   1.6%    91.0%       0.004s       5.51e-06s     C      798       2   theano.gpuarray.blas.GpuGemm
   1.4%    92.5%       0.004s       8.56e-06s     C      464       2   theano.gpuarray.basic_ops.GpuJoin
   1.0%    93.4%       0.003s       3.91e-07s     C     6960      30   theano.tensor.elemwise.Elemwise
   0.9%    94.4%       0.003s       5.61e-06s     C      464       2   theano.tensor.basic.Argmax
   0.9%    95.3%       0.002s       1.39e-06s     C     1791       7   theano.gpuarray.basic_ops.GpuReshape
   0.8%    96.1%       0.002s       9.98e-07s     C     2125       7   theano.gpuarray.basic_ops.GpuAllocEmpty
   0.7%    96.7%       0.002s       2.28e-07s     C     7962      30   theano.compile.ops.Shape_i
   0.6%    97.3%       0.002s       1.86e-07s     C     8778      22   theano.tensor.subtensor.Subtensor
   0.5%    97.8%       0.001s       3.46e-06s     C      399       1   theano.gpuarray.nnet.GpuCrossentropySoftmax1HotWithBiasDx
   ... (remaining 10 Classes account for   2.18%(0.01s) of the runtime)

Ops
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Op name>
  44.1%    44.1%       0.121s       8.73e-05s     C     1392        6   HostFromGpu(gpuarray)
   5.0%    49.1%       0.014s       2.98e-05s     Py     464        2   Unique{return_index=False, return_inverse=False, return_counts=False}
   4.7%    53.8%       0.013s       1.61e-05s     C      798        2   GpuDnnConvGradW{algo='none', inplace=True}
   4.1%    57.9%       0.011s       4.49e-06s     C     2524        8   GpuFromHost<None>
   4.1%    62.0%       0.011s       1.21e-05s     C      928        4   GpuDnnConv{algo='small', inplace=True}
   3.9%    65.9%       0.011s       6.20e-06s     C     1726        6   GpuDot22
   3.0%    68.9%       0.008s       1.76e-05s     Py     464        2   dot
   2.7%    71.5%       0.007s       1.83e-05s     C      399        1   GpuDnnConvGradI{algo='none', inplace=True}
   2.6%    74.1%       0.007s       3.57e-06s     C     1995        5   GpuElemwise{Composite{(i0 - (i1 * i2))}}[(0, 0)]<gpuarray>
   1.6%    75.7%       0.004s       5.51e-06s     C      798        2   GpuGemm{inplace=True}
   1.5%    77.2%       0.004s       8.94e-06s     C      464        2   GpuElemwise{Cast{int32}}[]<gpuarray>
   1.4%    78.6%       0.004s       8.56e-06s     C      464        2   GpuJoin
   1.3%    79.9%       0.004s       3.90e-06s     C      928        4   GpuElemwise{Add}[(0, 0)]<gpuarray>
   1.3%    81.2%       0.003s       7.43e-06s     C      464        2   GpuElemwise{Composite{Cast{int64}(EQ(i0, i1))}}[]<gpuarray>
   1.1%    82.3%       0.003s       3.88e-06s     C      798        2   GpuCAReduceCuda{add}{0, 2, 3}
   1.0%    83.3%       0.003s       5.66e-06s     C      464        2   GpuElemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)]<gpuarray>
   0.9%    84.2%       0.003s       5.61e-06s     C      464        2   Argmax
   0.9%    85.1%       0.002s       3.09e-06s     C      798        2   GpuCAReduceCuda{add}{0}
   0.8%    85.9%       0.002s       9.98e-07s     C     2125        7   GpuAllocEmpty{dtype='float32', context_name=None}
   0.7%    86.6%       0.002s       4.90e-06s     C      399        1   GpuElemwise{Composite{(i0 * i1 * (i2 - i1))}}[(0, 0)]<gpuarray>
   ... (remaining 46 Ops account for  13.40%(0.04s) of the runtime)

Apply
------
<% time> <sum %> <apply time> <time per call> <#call> <id> <Apply name>
  38.3%    38.3%       0.105s       2.64e-04s    399   101   HostFromGpu(gpuarray)(GpuElemwise{Add}[(0, 0)]<gpuarray>.0)
   4.3%    42.6%       0.012s       2.99e-05s    399   118   Unique{return_index=False, return_inverse=False, return_counts=False}(HostFromGpu(gpuarray).0)
   2.7%    45.3%       0.007s       1.83e-05s    399   145   GpuDnnConvGradI{algo='none', inplace=True}(GpuContiguous.0, GpuContiguous.0, GpuAllocEmpty{dtype='float32', context_name=None}.0, GpuDnnConvDesc{border_mode='valid', subsample=(2, 2), conv_mode='conv', precision='float32'}.0, Constant{1.0}, Constant{0.0})
   2.5%    47.8%       0.007s       1.76e-05s    399   144   dot(HostFromGpu(gpuarray).0, Elemwise{Composite{Cast{int64}(EQ(i0, i1))}}.0)
   2.5%    50.3%       0.007s       1.74e-05s    399   161   GpuDnnConvGradW{algo='none', inplace=True}(GpuContiguous.0, GpuContiguous.0, GpuAllocEmpty{dtype='float32', context_name=None}.0, GpuDnnConvDesc{border_mode='valid', subsample=(2, 2), conv_mode='conv', precision='float32'}.0, Constant{1.0}, Constant{0.0})
   2.2%    52.5%       0.006s       1.48e-05s    399   146   GpuDnnConvGradW{algo='none', inplace=True}(GpuContiguous.0, GpuContiguous.0, GpuAllocEmpty{dtype='float32', context_name=None}.0, GpuDnnConvDesc{border_mode='valid', subsample=(2, 2), conv_mode='conv', precision='float32'}.0, Constant{0.3}, Constant{0.0})
   2.1%    54.6%       0.006s       9.08e-05s     65    67   HostFromGpu(gpuarray)(GpuElemwise{Add}[(0, 0)]<gpuarray>.0)
   2.1%    56.7%       0.006s       1.43e-05s    399    72   GpuDnnConv{algo='small', inplace=True}(GpuContiguous.0, GpuContiguous.0, GpuAllocEmpty{dtype='float32', context_name=None}.0, GpuDnnConvDesc{border_mode='valid', subsample=(2, 2), conv_mode='conv', precision='float32'}.0, Constant{1.0}, Constant{0.0})
   1.8%    58.5%       0.005s       1.26e-05s    399   141   HostFromGpu(gpuarray)(GpuElemwise{Composite{Cast{int64}(EQ(i0, i1))}}[]<gpuarray>.0)
   1.4%    60.0%       0.004s       9.84e-06s    399    89   GpuDnnConv{algo='small', inplace=True}(GpuContiguous.0, GpuContiguous.0, GpuAllocEmpty{dtype='float32', context_name=None}.0, GpuDnnConvDesc{border_mode='valid', subsample=(2, 2), conv_mode='conv', precision='float32'}.0, Constant{1.0}, Constant{0.0})
   1.3%    61.3%       0.004s       9.27e-06s    399   116   HostFromGpu(gpuarray)(GpuJoin.0)
   1.3%    62.6%       0.004s       8.99e-06s    399    49   GpuElemwise{Cast{int32}}[]<gpuarray>(GpuSubtensor{int64:int64:}.0)
   1.2%    63.9%       0.003s       8.58e-06s    399   112   GpuJoin(TensorConstant{0}, GpuElemwise{Cast{int32}}[]<gpuarray>.0, GpuFromHost<None>.0)
   1.1%    64.9%       0.003s       7.44e-06s    399   131   GpuElemwise{Composite{Cast{int64}(EQ(i0, i1))}}[]<gpuarray>(InplaceGpuDimShuffle{x,0}.0, InplaceGpuDimShuffle{0,x}.0)
   1.1%    66.0%       0.003s       7.30e-06s    399   103   GpuDot22(GpuCrossentropySoftmax1HotWithBiasDx.0, InplaceGpuDimShuffle{1,0}.0)
   1.0%    67.0%       0.003s       7.18e-06s    399    93   GpuDot22(Gpusigmoid.0, W)
   0.9%    68.0%       0.003s       6.51e-06s    399   123   GpuFromHost<None>(Unique{return_index=False, return_inverse=False, return_counts=False}.0)
   0.9%    68.9%       0.003s       6.38e-06s    399   113   GpuGemm{inplace=True}(W, TensorConstant{-0.30000001192092896}, InplaceGpuDimShuffle{1,0}.0, GpuElemwise{Composite{(i0 * (i1 - sqr(i2)))}}[(0, 0)]<gpuarray>.0, TensorConstant{1.0})
   0.9%    69.8%       0.002s       5.95e-06s    399   108   GpuFromHost<None>(argmax)
   0.8%    70.6%       0.002s       5.62e-06s    399    73   GpuElemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)]<gpuarray>(GpuDnnConv{algo='small', inplace=True}.0, InplaceGpuDimShuffle{x,0,x,x}.0)
   ... (remaining 225 Apply instances account for 29.42%(0.08s) of the runtime)

Here are tips to potentially make your code run faster
                 (if you think of new ones, suggest them on the mailing list).
                 Test them first, as they are not guaranteed to always provide a speedup.
  - Try installing amdlibm and set the Theano flag lib.amdlibm=True. This speeds up only some Elemwise operation.
  - You have a dot operation that was not optimized to dot22 (which is faster). Make sure the inputs are float32 or float64, and are the same for both inputs. Currently they are: [TensorType(int64, matrix), TensorType(int64, matrix)]
  - You have a dot operation that was not optimized to dot22 (which is faster). Make sure the inputs are float32 or float64, and are the same for both inputs. Currently they are: [TensorType(int64, matrix), TensorType(int64, matrix)]
Function profiling
==================
  Message: /home/hal/Documentos/Felipe/Lenet5/lenet_5/Lenet5.py:92
  Time in 399 calls to Function.__call__: 5.493706e+00s
  Time in Function.fn.__call__: 5.466020e+00s (99.496%)
  Time in thunks: 5.447814e+00s (99.165%)
  Total compile time: 1.462894e+01s
    Number of Apply nodes: 86
    Theano Optimizer time: 8.799624e-01s
       Theano validate time: 1.189184e-02s
    Theano Linker time (includes C, CUDA code generation/compiling): 1.340480e+01s
       Import time 3.682375e-02s
       Node make_thunk time 1.340074e+01s
           Node Elemwise{Composite{(i0 * i1 * (i2 - i1))}}[(0, 0)](CorrMM_gradInputs{valid, (2, 2), (1, 1)}.0, Elemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)].0, TensorConstant{(1, 1, 1, 1) of 1.0}) time 4.614115e-01s
           Node Elemwise{Composite{(i0 - (i1 * i2))}}[(0, 0)](<TensorType(float32, 4D)>, TensorConstant{(1, 1, 1, 1) of 0.3}, Subtensor{::, ::, ::int64, ::int64}.0) time 4.445693e-01s
           Node CorrMM_gradInputs{valid, (2, 2), (1, 1)}(Subtensor{::, ::, ::int64, ::int64}.0, Elemwise{Composite{(i0 * scalar_sigmoid(i1) * (i2 - scalar_sigmoid(i1)))}}[(0, 0)].0, Subtensor{int64}.0, Subtensor{int64}.0) time 4.444921e-01s
           Node Elemwise{Composite{(i0 * scalar_sigmoid(i1) * (i2 - scalar_sigmoid(i1)))}}[(0, 0)](Reshape{4}.0, Elemwise{Add}[(0, 0)].0, TensorConstant{(1, 1, 1, 1) of 1.0}) time 4.442263e-01s
           Node CorrMM_gradWeights{valid, (2, 2), (1, 1)}(Elemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)].0, Elemwise{Composite{(i0 * scalar_sigmoid(i1) * (i2 - scalar_sigmoid(i1)))}}[(0, 0)].0, Subtensor{int64}.0, Subtensor{int64}.0) time 4.441843e-01s

Time in all call to theano.grad() 1.666379e-02s
Time since theano import 22.859s
Class
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Class name>
  45.3%    45.3%       2.468s       2.69e-04s     C     9177      23   theano.tensor.elemwise.Elemwise
  20.1%    65.4%       1.096s       1.37e-03s     C      798       2   theano.tensor.nnet.corr.CorrMM_gradWeights
  17.9%    83.4%       0.977s       1.22e-03s     C      798       2   theano.tensor.nnet.corr.CorrMM
  12.1%    95.5%       0.659s       1.65e-03s     C      399       1   theano.tensor.nnet.corr.CorrMM_gradInputs
   2.1%    97.5%       0.113s       7.05e-05s     C     1596       4   theano.tensor.elemwise.Sum
   1.1%    98.6%       0.057s       3.60e-05s     C     1596       4   theano.tensor.blas.Dot22
   0.4%    99.0%       0.024s       3.06e-05s     C      798       2   theano.tensor.blas.Gemm
   0.3%    99.3%       0.014s       3.61e-05s     C      399       1   theano.tensor.nnet.nnet.SoftmaxWithBias
   0.3%    99.6%       0.014s       3.43e-05s     Py     399       1   theano.tensor.extra_ops.Unique
   0.1%    99.7%       0.007s       1.70e-05s     Py     399       1   theano.tensor.basic.Dot
   0.1%    99.8%       0.005s       3.34e-06s     PyC     1596       4   theano.tensor.basic.Reshape
   0.1%    99.8%       0.003s       4.88e-07s     C     5985      15   theano.tensor.elemwise.DimShuffle
   0.1%    99.9%       0.003s       5.91e-07s     C     4788      12   theano.tensor.subtensor.Subtensor
   0.0%    99.9%       0.002s       5.67e-06s     C      399       1   theano.tensor.basic.Argmax
   0.0%   100.0%       0.002s       3.77e-06s     C      399       1   theano.tensor.basic.Join
   0.0%   100.0%       0.001s       4.08e-07s     C     2394       6   theano.compile.ops.Shape_i
   0.0%   100.0%       0.001s       2.27e-06s     C      399       1   theano.tensor.nnet.nnet.CrossentropySoftmax1HotWithBiasDx
   0.0%   100.0%       0.000s       4.90e-07s     C      798       2   theano.tensor.opt.MakeVector
   0.0%   100.0%       0.000s       3.16e-07s     C      798       2   theano.tensor.basic.ScalarFromTensor
   0.0%   100.0%       0.000s       2.84e-07s     C      399       1   theano.compile.ops.Rebroadcast
   ... (remaining 0 Classes account for   0.00%(0.00s) of the runtime)

Ops
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Op name>
  24.1%    24.1%       1.312s       3.29e-03s     C      399        1   Elemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)]
  20.1%    44.2%       1.096s       1.37e-03s     C      798        2   CorrMM_gradWeights{valid, (2, 2), (1, 1)}
  17.9%    62.1%       0.977s       1.22e-03s     C      798        2   CorrMM{valid, (2, 2), (1, 1)}
  12.1%    74.2%       0.659s       1.65e-03s     C      399        1   CorrMM_gradInputs{valid, (2, 2), (1, 1)}
   8.9%    83.1%       0.482s       1.21e-03s     C      399        1   Elemwise{Composite{(i0 * scalar_sigmoid(i1) * (i2 - scalar_sigmoid(i1)))}}[(0, 0)]
   8.4%    91.4%       0.455s       1.14e-03s     C      399        1   sigmoid
   3.0%    94.5%       0.166s       4.16e-04s     C      399        1   Elemwise{Composite{tanh((i0 + i1))}}[(0, 0)]
   1.9%    96.4%       0.105s       1.32e-04s     C      798        2   Sum{axis=[0, 2, 3], acc_dtype=float64}
   1.1%    97.5%       0.057s       3.60e-05s     C     1596        4   Dot22
   0.4%    97.9%       0.024s       3.06e-05s     C      798        2   Gemm{inplace}
   0.4%    98.4%       0.024s       3.06e-05s     C      798        2   Elemwise{Add}[(0, 0)]
   0.3%    98.7%       0.019s       4.70e-05s     C      399        1   Elemwise{Composite{(i0 * i1 * (i2 - i1))}}[(0, 0)]
   0.3%    99.0%       0.014s       3.61e-05s     C      399        1   SoftmaxWithBias
   0.3%    99.2%       0.014s       3.43e-05s     Py     399        1   Unique{return_index=False, return_inverse=False, return_counts=False}
   0.1%    99.4%       0.007s       9.13e-06s     C      798        2   Sum{axis=[0], acc_dtype=float64}
   0.1%    99.5%       0.007s       1.70e-05s     Py     399        1   dot
   0.1%    99.6%       0.004s       1.09e-05s     Py     399        1   Reshape{1}
   0.1%    99.6%       0.003s       1.20e-06s     C     2394        6   Elemwise{Composite{(i0 - (i1 * i2))}}[(0, 0)]
   0.1%    99.7%       0.003s       7.07e-06s     C      399        1   Elemwise{Composite{(i0 * (i1 - sqr(i2)))}}[(0, 0)]
   0.0%    99.7%       0.003s       3.16e-06s     C      798        2   Elemwise{Composite{Cast{int64}(EQ(i0, i1))}}
   ... (remaining 29 Ops account for   0.27%(0.01s) of the runtime)

Apply
------
<% time> <sum %> <apply time> <time per call> <#call> <id> <Apply name>
  24.1%    24.1%       1.312s       3.29e-03s    399    40   Elemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)](CorrMM{valid, (2, 2), (1, 1)}.0, InplaceDimShuffle{x,0,x,x}.0)
  12.1%    36.2%       0.659s       1.65e-03s    399    74   CorrMM_gradInputs{valid, (2, 2), (1, 1)}(Subtensor{::, ::, ::int64, ::int64}.0, Elemwise{Composite{(i0 * scalar_sigmoid(i1) * (i2 - scalar_sigmoid(i1)))}}[(0, 0)].0, Subtensor{int64}.0, Subtensor{int64}.0)
  10.5%    46.7%       0.573s       1.44e-03s    399    75   CorrMM_gradWeights{valid, (2, 2), (1, 1)}(Elemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)].0, Elemwise{Composite{(i0 * scalar_sigmoid(i1) * (i2 - scalar_sigmoid(i1)))}}[(0, 0)].0, Subtensor{int64}.0, Subtensor{int64}.0)
  10.2%    56.9%       0.555s       1.39e-03s    399    41   CorrMM{valid, (2, 2), (1, 1)}(Elemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)].0, Subtensor{::, ::, ::int64, ::int64}.0)
   9.6%    66.5%       0.523s       1.31e-03s    399    80   CorrMM_gradWeights{valid, (2, 2), (1, 1)}(InplaceDimShuffle{0,x,1,2}.0, Elemwise{Composite{(i0 * i1 * (i2 - i1))}}[(0, 0)].0, Subtensor{int64}.0, Subtensor{int64}.0)
   8.9%    75.3%       0.482s       1.21e-03s    399    71   Elemwise{Composite{(i0 * scalar_sigmoid(i1) * (i2 - scalar_sigmoid(i1)))}}[(0, 0)](Reshape{4}.0, Elemwise{Add}[(0, 0)].0, TensorConstant{(1, 1, 1, 1) of 1.0})
   8.4%    83.7%       0.455s       1.14e-03s    399    44   sigmoid(Reshape{2}.0)
   7.7%    91.4%       0.422s       1.06e-03s    399    39   CorrMM{valid, (2, 2), (1, 1)}(InplaceDimShuffle{0,x,1,2}.0, Subtensor{::, ::, ::int64, ::int64}.0)
   3.0%    94.5%       0.166s       4.16e-04s    399    47   Elemwise{Composite{tanh((i0 + i1))}}[(0, 0)](Dot22.0, InplaceDimShuffle{x,0}.0)
   1.4%    95.9%       0.074s       1.86e-04s    399    79   Sum{axis=[0, 2, 3], acc_dtype=float64}(Elemwise{Composite{(i0 * i1 * (i2 - i1))}}[(0, 0)].0)
   0.6%    96.4%       0.031s       7.78e-05s    399    73   Sum{axis=[0, 2, 3], acc_dtype=float64}(Elemwise{Composite{(i0 * scalar_sigmoid(i1) * (i2 - scalar_sigmoid(i1)))}}[(0, 0)].0)
   0.5%    97.0%       0.029s       7.27e-05s    399    45   Dot22(sigmoid.0, W)
   0.4%    97.4%       0.024s       5.92e-05s    399    42   Elemwise{Add}[(0, 0)](CorrMM{valid, (2, 2), (1, 1)}.0, InplaceDimShuffle{x,0,x,x}.0)
   0.4%    97.8%       0.022s       5.58e-05s    399    63   Dot22(Elemwise{Composite{(i0 * (i1 - sqr(i2)))}}[(0, 0)].0, W.T)
   0.4%    98.2%       0.022s       5.48e-05s    399    66   Gemm{inplace}(W, TensorConstant{-0.30000001192092896}, InplaceDimShuffle{1,0}.0, Elemwise{Composite{(i0 * (i1 - sqr(i2)))}}[(0, 0)].0, TensorConstant{1.0})
   0.3%    98.5%       0.019s       4.70e-05s    399    77   Elemwise{Composite{(i0 * i1 * (i2 - i1))}}[(0, 0)](CorrMM_gradInputs{valid, (2, 2), (1, 1)}.0, Elemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)].0, TensorConstant{(1, 1, 1, 1) of 1.0})
   0.3%    98.8%       0.014s       3.61e-05s    399    50   SoftmaxWithBias(Dot22.0, b)
   0.3%    99.1%       0.014s       3.43e-05s    399    61   Unique{return_index=False, return_inverse=False, return_counts=False}(Join.0)
   0.1%    99.2%       0.007s       1.70e-05s    399    72   dot(Elemwise{Composite{Cast{int64}(EQ(i0, i1))}}.0, Elemwise{Composite{Cast{int64}(EQ(i0, i1))}}.0)
   0.1%    99.3%       0.006s       1.62e-05s    399    62   Sum{axis=[0], acc_dtype=float64}(Elemwise{Composite{(i0 * (i1 - sqr(i2)))}}[(0, 0)].0)
   ... (remaining 66 Apply instances account for 0.69%(0.04s) of the runtime)

Here are tips to potentially make your code run faster
                 (if you think of new ones, suggest them on the mailing list).
                 Test them first, as they are not guaranteed to always provide a speedup.
  - Try installing amdlibm and set the Theano flag lib.amdlibm=True. This speeds up only some Elemwise operation.
  - You have a dot operation that was not optimized to dot22 (which is faster). Make sure the inputs are float32 or float64, and are the same for both inputs. Currently they are: [TensorType(int64, matrix), TensorType(int64, matrix)]
Function profiling
==================
  Message: /home/hal/Documentos/Felipe/Lenet5/lenet_5/Lenet5.py:111
  Time in 65 calls to Function.__call__: 4.918540e-01s
  Time in Function.fn.__call__: 4.889004e-01s (99.400%)
  Time in thunks: 4.878535e-01s (99.187%)
  Total compile time: 7.505658e-01s
    Number of Apply nodes: 36
    Theano Optimizer time: 1.127574e-01s
       Theano validate time: 1.502275e-03s
    Theano Linker time (includes C, CUDA code generation/compiling): 6.290112e-01s
       Import time 1.633644e-03s
       Node make_thunk time 6.275029e-01s
           Node Elemwise{ScalarSigmoid}[(0, 0)](Reshape{2}.0) time 3.828197e-01s
           Node InplaceDimShuffle{x,0,x,x}(<TensorType(float32, vector)>) time 2.276711e-01s
           Node InplaceDimShuffle{x,0}(b) time 1.590490e-03s
           Node Subtensor{::, ::, ::int64, ::int64}(<TensorType(float32, 4D)>, Constant{-1}, Constant{-1}) time 7.791519e-04s
           Node Elemwise{Composite{(i0 * (i1 + i2))}}(TensorConstant{150}, TensorConstant{1}, <TensorType(int64, scalar)>) time 7.286072e-04s

Time in all call to theano.grad() 1.666379e-02s
Time since theano import 22.875s
Class
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Class name>
  65.5%    65.5%       0.319s       4.91e-04s     C      650      10   theano.tensor.elemwise.Elemwise
  32.6%    98.0%       0.159s       1.22e-03s     C      130       2   theano.tensor.nnet.corr.CorrMM
   1.1%    99.1%       0.005s       3.95e-05s     C      130       2   theano.tensor.blas.Dot22
   0.4%    99.5%       0.002s       2.87e-05s     Py      65       1   theano.tensor.extra_ops.Unique
   0.2%    99.7%       0.001s       1.58e-05s     Py      65       1   theano.tensor.basic.Dot
   0.1%    99.8%       0.001s       2.85e-06s     PyC      195       3   theano.tensor.basic.Reshape
   0.1%    99.9%       0.000s       5.85e-06s     C       65       1   theano.tensor.basic.Argmax
   0.0%    99.9%       0.000s       3.64e-07s     C      585       9   theano.tensor.elemwise.DimShuffle
   0.0%   100.0%       0.000s       3.04e-06s     C       65       1   theano.tensor.basic.Join
   0.0%   100.0%       0.000s       5.80e-07s     C      260       4   theano.tensor.subtensor.Subtensor
   0.0%   100.0%       0.000s       2.24e-07s     C      130       2   theano.tensor.basic.ScalarFromTensor
   ... (remaining 0 Classes account for   0.00%(0.00s) of the runtime)

Ops
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Op name>
  44.0%    44.0%       0.214s       3.30e-03s     C       65        1   Elemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)]
  32.6%    76.5%       0.159s       1.22e-03s     C      130        2   CorrMM{valid, (2, 2), (1, 1)}
  14.7%    91.3%       0.072s       1.11e-03s     C       65        1   Elemwise{ScalarSigmoid}[(0, 0)]
   5.9%    97.2%       0.029s       4.43e-04s     C       65        1   Elemwise{Composite{tanh((i0 + i1))}}[(0, 0)]
   1.1%    98.2%       0.005s       3.95e-05s     C      130        2   Dot22
   0.8%    99.0%       0.004s       3.03e-05s     C      130        2   Elemwise{Add}[(0, 0)]
   0.4%    99.4%       0.002s       2.87e-05s     Py      65        1   Unique{return_index=False, return_inverse=False, return_counts=False}
   0.2%    99.6%       0.001s       1.58e-05s     Py      65        1   dot
   0.1%    99.7%       0.000s       7.45e-06s     Py      65        1   Reshape{1}
   0.1%    99.8%       0.000s       5.85e-06s     C       65        1   Argmax
   0.1%    99.8%       0.000s       2.53e-06s     C      130        2   Elemwise{Composite{Cast{int64}(EQ(i0, i1))}}
   0.0%    99.9%       0.000s       3.04e-06s     C       65        1   Join
   0.0%    99.9%       0.000s       6.66e-07s     C      130        2   Subtensor{::, ::, ::int64, ::int64}
   0.0%    99.9%       0.000s       2.93e-07s     C      260        4   InplaceDimShuffle{x,0}
   0.0%    99.9%       0.000s       5.03e-07s     C      130        2   InplaceDimShuffle{x,0,x,x}
   0.0%    99.9%       0.000s       4.95e-07s     C      130        2   Subtensor{int64:int64:}
   0.0%   100.0%       0.000s       3.12e-07s     C      130        2   InplaceDimShuffle{0,x}
   0.0%   100.0%       0.000s       5.80e-07s     C       65        1   Reshape{2}
   0.0%   100.0%       0.000s       5.32e-07s     C       65        1   Reshape{3}
   0.0%   100.0%       0.000s       4.81e-07s     C       65        1   InplaceDimShuffle{0,x,1,2}
   ... (remaining 4 Ops account for   0.02%(0.00s) of the runtime)

Apply
------
<% time> <sum %> <apply time> <time per call> <#call> <id> <Apply name>
  44.0%    44.0%       0.214s       3.30e-03s     65    18   Elemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)](CorrMM{valid, (2, 2), (1, 1)}.0, InplaceDimShuffle{x,0,x,x}.0)
  18.5%    62.5%       0.090s       1.39e-03s     65    19   CorrMM{valid, (2, 2), (1, 1)}(Elemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)].0, Subtensor{::, ::, ::int64, ::int64}.0)
  14.7%    77.2%       0.072s       1.11e-03s     65    22   Elemwise{ScalarSigmoid}[(0, 0)](Reshape{2}.0)
  14.1%    91.3%       0.069s       1.05e-03s     65    17   CorrMM{valid, (2, 2), (1, 1)}(InplaceDimShuffle{0,x,1,2}.0, Subtensor{::, ::, ::int64, ::int64}.0)
   5.9%    97.2%       0.029s       4.43e-04s     65    24   Elemwise{Composite{tanh((i0 + i1))}}[(0, 0)](Dot22.0, InplaceDimShuffle{x,0}.0)
   1.0%    98.1%       0.005s       7.19e-05s     65    23   Dot22(Elemwise{ScalarSigmoid}[(0, 0)].0, W)
   0.8%    98.9%       0.004s       5.89e-05s     65    20   Elemwise{Add}[(0, 0)](CorrMM{valid, (2, 2), (1, 1)}.0, InplaceDimShuffle{x,0,x,x}.0)
   0.4%    99.3%       0.002s       2.87e-05s     65    30   Unique{return_index=False, return_inverse=False, return_counts=False}(Join.0)
   0.2%    99.5%       0.001s       1.58e-05s     65    35   dot(Elemwise{Composite{Cast{int64}(EQ(i0, i1))}}.0, Elemwise{Composite{Cast{int64}(EQ(i0, i1))}}.0)
   0.1%    99.6%       0.000s       7.45e-06s     65     8   Reshape{1}(<TensorType(float32, matrix)>, TensorConstant{(1,) of -1})
   0.1%    99.7%       0.000s       6.99e-06s     65    25   Dot22(Elemwise{Composite{tanh((i0 + i1))}}[(0, 0)].0, W)
   0.1%    99.8%       0.000s       5.85e-06s     65    27   Argmax(Elemwise{Add}[(0, 0)].0, TensorConstant{(1,) of 1})
   0.0%    99.8%       0.000s       3.04e-06s     65    28   Join(TensorConstant{0}, Elemwise{Cast{int32}}.0, argmax)
   0.0%    99.8%       0.000s       2.62e-06s     65    33   Elemwise{Composite{Cast{int64}(EQ(i0, i1))}}(InplaceDimShuffle{0,x}.0, InplaceDimShuffle{x,0}.0)
   0.0%    99.9%       0.000s       2.45e-06s     65    34   Elemwise{Composite{Cast{int64}(EQ(i0, i1))}}(InplaceDimShuffle{x,0}.0, InplaceDimShuffle{0,x}.0)
   0.0%    99.9%       0.000s       1.76e-06s     65    26   Elemwise{Add}[(0, 0)](Dot22.0, InplaceDimShuffle{x,0}.0)
   0.0%    99.9%       0.000s       7.56e-07s     65     3   Subtensor{::, ::, ::int64, ::int64}(<TensorType(float32, 4D)>, Constant{-1}, Constant{-1})
   0.0%    99.9%       0.000s       6.20e-07s     65    12   Subtensor{int64:int64:}(Reshape{1}.0, ScalarFromTensor.0, ScalarFromTensor.0)
   0.0%    99.9%       0.000s       5.80e-07s     65    21   Reshape{2}(Elemwise{Add}[(0, 0)].0, TensorConstant{[150  -1]})
   0.0%    99.9%       0.000s       5.80e-07s     65     4   InplaceDimShuffle{x,0,x,x}(<TensorType(float32, vector)>)
   ... (remaining 16 Apply instances account for 0.08%(0.00s) of the runtime)

Here are tips to potentially make your code run faster
                 (if you think of new ones, suggest them on the mailing list).
                 Test them first, as they are not guaranteed to always provide a speedup.
  - Try installing amdlibm and set the Theano flag lib.amdlibm=True. This speeds up only some Elemwise operation.
  - You have a dot operation that was not optimized to dot22 (which is faster). Make sure the inputs are float32 or float64, and are the same for both inputs. Currently they are: [TensorType(int64, matrix), TensorType(int64, matrix)]
Function profiling
==================
  Message: Sum of all(2) printed profiles at exit excluding Scan op profile.
  Time in 464 calls to Function.__call__: 5.985560e+00s
  Time in Function.fn.__call__: 5.954920e+00s (99.488%)
  Time in thunks: 5.935668e+00s (99.166%)
  Total compile time: 1.537950e+01s
    Number of Apply nodes: 86
    Theano Optimizer time: 9.927199e-01s
       Theano validate time: 1.339412e-02s
    Theano Linker time (includes C, CUDA code generation/compiling): 1.403381e+01s
       Import time 3.845739e-02s
       Node make_thunk time 1.402825e+01s
           Node Elemwise{Composite{(i0 * i1 * (i2 - i1))}}[(0, 0)](CorrMM_gradInputs{valid, (2, 2), (1, 1)}.0, Elemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)].0, TensorConstant{(1, 1, 1, 1) of 1.0}) time 4.614115e-01s
           Node Elemwise{Composite{(i0 - (i1 * i2))}}[(0, 0)](<TensorType(float32, 4D)>, TensorConstant{(1, 1, 1, 1) of 0.3}, Subtensor{::, ::, ::int64, ::int64}.0) time 4.445693e-01s
           Node CorrMM_gradInputs{valid, (2, 2), (1, 1)}(Subtensor{::, ::, ::int64, ::int64}.0, Elemwise{Composite{(i0 * scalar_sigmoid(i1) * (i2 - scalar_sigmoid(i1)))}}[(0, 0)].0, Subtensor{int64}.0, Subtensor{int64}.0) time 4.444921e-01s
           Node Elemwise{Composite{(i0 * scalar_sigmoid(i1) * (i2 - scalar_sigmoid(i1)))}}[(0, 0)](Reshape{4}.0, Elemwise{Add}[(0, 0)].0, TensorConstant{(1, 1, 1, 1) of 1.0}) time 4.442263e-01s
           Node CorrMM_gradWeights{valid, (2, 2), (1, 1)}(Elemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)].0, Elemwise{Composite{(i0 * scalar_sigmoid(i1) * (i2 - scalar_sigmoid(i1)))}}[(0, 0)].0, Subtensor{int64}.0, Subtensor{int64}.0) time 4.441843e-01s

Time in all call to theano.grad() 1.666379e-02s
Time since theano import 22.883s
Class
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Class name>
  47.0%    47.0%       2.788s       2.84e-04s     C     9827      33   theano.tensor.elemwise.Elemwise
  19.1%    66.1%       1.136s       1.22e-03s     C      928       4   theano.tensor.nnet.corr.CorrMM
  18.5%    84.6%       1.096s       1.37e-03s     C      798       2   theano.tensor.nnet.corr.CorrMM_gradWeights
  11.1%    95.7%       0.659s       1.65e-03s     C      399       1   theano.tensor.nnet.corr.CorrMM_gradInputs
   1.9%    97.6%       0.113s       7.05e-05s     C     1596       4   theano.tensor.elemwise.Sum
   1.1%    98.6%       0.063s       3.62e-05s     C     1726       6   theano.tensor.blas.Dot22
   0.4%    99.0%       0.024s       3.06e-05s     C      798       2   theano.tensor.blas.Gemm
   0.3%    99.3%       0.016s       3.35e-05s     Py     464       2   theano.tensor.extra_ops.Unique
   0.2%    99.5%       0.014s       3.61e-05s     C      399       1   theano.tensor.nnet.nnet.SoftmaxWithBias
   0.1%    99.7%       0.008s       1.69e-05s     Py     464       2   theano.tensor.basic.Dot
   0.1%    99.8%       0.006s       3.29e-06s     PyC     1791       7   theano.tensor.basic.Reshape
   0.1%    99.8%       0.003s       4.77e-07s     C     6570      24   theano.tensor.elemwise.DimShuffle
   0.1%    99.9%       0.003s       5.90e-07s     C     5048      16   theano.tensor.subtensor.Subtensor
   0.0%    99.9%       0.003s       5.69e-06s     C      464       2   theano.tensor.basic.Argmax
   0.0%   100.0%       0.002s       3.67e-06s     C      464       2   theano.tensor.basic.Join
   0.0%   100.0%       0.001s       4.08e-07s     C     2394       6   theano.compile.ops.Shape_i
   0.0%   100.0%       0.001s       2.27e-06s     C      399       1   theano.tensor.nnet.nnet.CrossentropySoftmax1HotWithBiasDx
   0.0%   100.0%       0.000s       4.90e-07s     C      798       2   theano.tensor.opt.MakeVector
   0.0%   100.0%       0.000s       3.03e-07s     C      928       4   theano.tensor.basic.ScalarFromTensor
   0.0%   100.0%       0.000s       2.84e-07s     C      399       1   theano.compile.ops.Rebroadcast
   ... (remaining 0 Classes account for   0.00%(0.00s) of the runtime)

Ops
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Op name>
  25.7%    25.7%       1.526s       3.29e-03s     C      464        2   Elemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)]
  19.1%    44.9%       1.136s       1.22e-03s     C      928        4   CorrMM{valid, (2, 2), (1, 1)}
  18.5%    63.3%       1.096s       1.37e-03s     C      798        2   CorrMM_gradWeights{valid, (2, 2), (1, 1)}
  11.1%    74.4%       0.659s       1.65e-03s     C      399        1   CorrMM_gradInputs{valid, (2, 2), (1, 1)}
   8.1%    82.6%       0.482s       1.21e-03s     C      399        1   Elemwise{Composite{(i0 * scalar_sigmoid(i1) * (i2 - scalar_sigmoid(i1)))}}[(0, 0)]
   7.7%    90.2%       0.455s       1.14e-03s     C      399        1   sigmoid
   3.3%    93.5%       0.195s       4.20e-04s     C      464        2   Elemwise{Composite{tanh((i0 + i1))}}[(0, 0)]
   1.8%    95.3%       0.105s       1.32e-04s     C      798        2   Sum{axis=[0, 2, 3], acc_dtype=float64}
   1.2%    96.5%       0.072s       1.11e-03s     C       65        1   Elemwise{ScalarSigmoid}[(0, 0)]
   1.1%    97.5%       0.063s       3.62e-05s     C     1726        6   Dot22
   0.5%    98.0%       0.028s       3.06e-05s     C      928        4   Elemwise{Add}[(0, 0)]
   0.4%    98.4%       0.024s       3.06e-05s     C      798        2   Gemm{inplace}
   0.3%    98.7%       0.019s       4.70e-05s     C      399        1   Elemwise{Composite{(i0 * i1 * (i2 - i1))}}[(0, 0)]
   0.3%    99.0%       0.016s       3.35e-05s     Py     464        2   Unique{return_index=False, return_inverse=False, return_counts=False}
   0.2%    99.3%       0.014s       3.61e-05s     C      399        1   SoftmaxWithBias
   0.1%    99.4%       0.008s       1.69e-05s     Py     464        2   dot
   0.1%    99.5%       0.007s       9.13e-06s     C      798        2   Sum{axis=[0], acc_dtype=float64}
   0.1%    99.6%       0.005s       1.04e-05s     Py     464        2   Reshape{1}
   0.0%    99.6%       0.003s       1.20e-06s     C     2394        6   Elemwise{Composite{(i0 - (i1 * i2))}}[(0, 0)]
   0.0%    99.7%       0.003s       3.07e-06s     C      928        4   Elemwise{Composite{Cast{int64}(EQ(i0, i1))}}
   ... (remaining 30 Ops account for   0.31%(0.02s) of the runtime)

Apply
------
<% time> <sum %> <apply time> <time per call> <#call> <id> <Apply name>
  22.1%    22.1%       1.312s       3.29e-03s    399    40   Elemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)](CorrMM{valid, (2, 2), (1, 1)}.0, InplaceDimShuffle{x,0,x,x}.0)
  11.1%    33.2%       0.659s       1.65e-03s    399    74   CorrMM_gradInputs{valid, (2, 2), (1, 1)}(Subtensor{::, ::, ::int64, ::int64}.0, Elemwise{Composite{(i0 * scalar_sigmoid(i1) * (i2 - scalar_sigmoid(i1)))}}[(0, 0)].0, Subtensor{int64}.0, Subtensor{int64}.0)
   9.7%    42.9%       0.573s       1.44e-03s    399    75   CorrMM_gradWeights{valid, (2, 2), (1, 1)}(Elemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)].0, Elemwise{Composite{(i0 * scalar_sigmoid(i1) * (i2 - scalar_sigmoid(i1)))}}[(0, 0)].0, Subtensor{int64}.0, Subtensor{int64}.0)
   9.4%    52.2%       0.555s       1.39e-03s    399    41   CorrMM{valid, (2, 2), (1, 1)}(Elemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)].0, Subtensor{::, ::, ::int64, ::int64}.0)
   8.8%    61.0%       0.523s       1.31e-03s    399    80   CorrMM_gradWeights{valid, (2, 2), (1, 1)}(InplaceDimShuffle{0,x,1,2}.0, Elemwise{Composite{(i0 * i1 * (i2 - i1))}}[(0, 0)].0, Subtensor{int64}.0, Subtensor{int64}.0)
   8.1%    69.2%       0.482s       1.21e-03s    399    71   Elemwise{Composite{(i0 * scalar_sigmoid(i1) * (i2 - scalar_sigmoid(i1)))}}[(0, 0)](Reshape{4}.0, Elemwise{Add}[(0, 0)].0, TensorConstant{(1, 1, 1, 1) of 1.0})
   7.7%    76.8%       0.455s       1.14e-03s    399    44   sigmoid(Reshape{2}.0)
   7.1%    83.9%       0.422s       1.06e-03s    399    39   CorrMM{valid, (2, 2), (1, 1)}(InplaceDimShuffle{0,x,1,2}.0, Subtensor{::, ::, ::int64, ::int64}.0)
   3.6%    87.5%       0.214s       3.30e-03s     65    18   Elemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)](CorrMM{valid, (2, 2), (1, 1)}.0, InplaceDimShuffle{x,0,x,x}.0)
   2.8%    90.3%       0.166s       4.16e-04s    399    47   Elemwise{Composite{tanh((i0 + i1))}}[(0, 0)](Dot22.0, InplaceDimShuffle{x,0}.0)
   1.5%    91.9%       0.090s       1.39e-03s     65    19   CorrMM{valid, (2, 2), (1, 1)}(Elemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)].0, Subtensor{::, ::, ::int64, ::int64}.0)
   1.3%    93.1%       0.074s       1.86e-04s    399    79   Sum{axis=[0, 2, 3], acc_dtype=float64}(Elemwise{Composite{(i0 * i1 * (i2 - i1))}}[(0, 0)].0)
   1.2%    94.3%       0.072s       1.11e-03s     65    22   Elemwise{ScalarSigmoid}[(0, 0)](Reshape{2}.0)
   1.2%    95.5%       0.069s       1.05e-03s     65    17   CorrMM{valid, (2, 2), (1, 1)}(InplaceDimShuffle{0,x,1,2}.0, Subtensor{::, ::, ::int64, ::int64}.0)
   0.5%    96.0%       0.031s       7.78e-05s    399    73   Sum{axis=[0, 2, 3], acc_dtype=float64}(Elemwise{Composite{(i0 * scalar_sigmoid(i1) * (i2 - scalar_sigmoid(i1)))}}[(0, 0)].0)
   0.5%    96.5%       0.029s       7.27e-05s    399    45   Dot22(sigmoid.0, W)
   0.5%    97.0%       0.029s       4.43e-04s     65    24   Elemwise{Composite{tanh((i0 + i1))}}[(0, 0)](Dot22.0, InplaceDimShuffle{x,0}.0)
   0.4%    97.4%       0.024s       5.92e-05s    399    42   Elemwise{Add}[(0, 0)](CorrMM{valid, (2, 2), (1, 1)}.0, InplaceDimShuffle{x,0,x,x}.0)
   0.4%    97.8%       0.022s       5.58e-05s    399    63   Dot22(Elemwise{Composite{(i0 * (i1 - sqr(i2)))}}[(0, 0)].0, W.T)
   0.4%    98.1%       0.022s       5.48e-05s    399    66   Gemm{inplace}(W, TensorConstant{-0.30000001192092896}, InplaceDimShuffle{1,0}.0, Elemwise{Composite{(i0 * (i1 - sqr(i2)))}}[(0, 0)].0, TensorConstant{1.0})
   ... (remaining 102 Apply instances account for 1.88%(0.11s) of the runtime)

Here are tips to potentially make your code run faster
                 (if you think of new ones, suggest them on the mailing list).
                 Test them first, as they are not guaranteed to always provide a speedup.
  - Try installing amdlibm and set the Theano flag lib.amdlibm=True. This speeds up only some Elemwise operation.
  - You have a dot operation that was not optimized to dot22 (which is faster). Make sure the inputs are float32 or float64, and are the same for both inputs. Currently they are: [TensorType(int64, matrix), TensorType(int64, matrix)]
  - You have a dot operation that was not optimized to dot22 (which is faster). Make sure the inputs are float32 or float64, and are the same for both inputs. Currently they are: [TensorType(int64, matrix), TensorType(int64, matrix)]
